{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310764c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:16:41.026380Z",
     "iopub.status.busy": "2024-11-11T08:16:41.026014Z",
     "iopub.status.idle": "2024-11-11T08:16:58.680329Z",
     "shell.execute_reply": "2024-11-11T08:16:58.679370Z"
    },
    "id": "Ge1CkjEpX5yI",
    "papermill": {
     "duration": 17.673095,
     "end_time": "2024-11-11T08:16:58.683017",
     "exception": false,
     "start_time": "2024-11-11T08:16:41.009922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from tokenizers.models import WordLevel\n",
    "\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7292f751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:16:58.713522Z",
     "iopub.status.busy": "2024-11-11T08:16:58.712991Z",
     "iopub.status.idle": "2024-11-11T08:18:24.471042Z",
     "shell.execute_reply": "2024-11-11T08:18:24.470079Z"
    },
    "papermill": {
     "duration": 85.775248,
     "end_time": "2024-11-11T08:18:24.473407",
     "exception": false,
     "start_time": "2024-11-11T08:16:58.698159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\r\n",
      "Channels:\r\n",
      " - rapidsai\r\n",
      " - nvidia\r\n",
      " - nodefaults\r\n",
      " - conda-forge\r\n",
      " - defaults\r\n",
      " - pytorch\r\n",
      "Platform: linux-64\r\n",
      "Collecting package metadata (repodata.json): \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\r\n",
      "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/conda\r\n",
      "\r\n",
      "  added / updated specs:\r\n",
      "    - gdown\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be downloaded:\r\n",
      "\r\n",
      "    package                    |            build\r\n",
      "    ---------------------------|-----------------\r\n",
      "    conda-24.9.2               |  py310hff52083_0         895 KB  conda-forge\r\n",
      "    filelock-3.16.1            |     pyhd8ed1ab_0          17 KB  conda-forge\r\n",
      "    gdown-5.2.0                |     pyhd8ed1ab_0          21 KB  conda-forge\r\n",
      "    ------------------------------------------------------------\r\n",
      "                                           Total:         933 KB\r\n",
      "\r\n",
      "The following NEW packages will be INSTALLED:\r\n",
      "\r\n",
      "  filelock           conda-forge/noarch::filelock-3.16.1-pyhd8ed1ab_0 \r\n",
      "  gdown              conda-forge/noarch::gdown-5.2.0-pyhd8ed1ab_0 \r\n",
      "\r\n",
      "The following packages will be UPDATED:\r\n",
      "\r\n",
      "  conda                              24.9.0-py310hff52083_0 --> 24.9.2-py310hff52083_0 \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages:\r\n",
      "conda-24.9.2         | 895 KB    |                                       |   0% \r\n",
      "gdown-5.2.0          | 21 KB     |                                       |   0% \u001b[A\r\n",
      "\r\n",
      "conda-24.9.2         | 895 KB    | ##6                                   |   7% \r\n",
      "\r\n",
      "filelock-3.16.1      | 17 KB     | ##################################### | 100% \u001b[A\u001b[A\r\n",
      "\r\n",
      "filelock-3.16.1      | 17 KB     | ##################################### | 100% \u001b[A\u001b[A\r\n",
      "gdown-5.2.0          | 21 KB     | ##################################### | 100% \u001b[A\r\n",
      "\r\n",
      "                                                                                \u001b[A\r\n",
      "\r\n",
      "\r\n",
      "Preparing transaction: - \b\bdone\r\n",
      "Verifying transaction: | \b\b/ \b\bdone\r\n",
      "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\r\n"
     ]
    }
   ],
   "source": [
    "!conda install -y gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92ed0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:24.528475Z",
     "iopub.status.busy": "2024-11-11T08:18:24.527700Z",
     "iopub.status.idle": "2024-11-11T08:18:36.745710Z",
     "shell.execute_reply": "2024-11-11T08:18:36.744714Z"
    },
    "papermill": {
     "duration": 12.247561,
     "end_time": "2024-11-11T08:18:36.748055",
     "exception": false,
     "start_time": "2024-11-11T08:18:24.500494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a87252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:36.803186Z",
     "iopub.status.busy": "2024-11-11T08:18:36.802865Z",
     "iopub.status.idle": "2024-11-11T08:18:36.807257Z",
     "shell.execute_reply": "2024-11-11T08:18:36.806424Z"
    },
    "papermill": {
     "duration": 0.034024,
     "end_time": "2024-11-11T08:18:36.809124",
     "exception": false,
     "start_time": "2024-11-11T08:18:36.775100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable parallelism in tokenization\n",
    "transformers.utils.logging.enable_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc3039",
   "metadata": {
    "id": "s9EjH3LtYJXR",
    "papermill": {
     "duration": 0.026018,
     "end_time": "2024-11-11T08:18:36.861477",
     "exception": false,
     "start_time": "2024-11-11T08:18:36.835459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7484d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:36.914654Z",
     "iopub.status.busy": "2024-11-11T08:18:36.914346Z",
     "iopub.status.idle": "2024-11-11T08:18:36.920428Z",
     "shell.execute_reply": "2024-11-11T08:18:36.919529Z"
    },
    "id": "vHaeRuqhX9I_",
    "papermill": {
     "duration": 0.034762,
     "end_time": "2024-11-11T08:18:36.922260",
     "exception": false,
     "start_time": "2024-11-11T08:18:36.887498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model:int, vocab_size:int):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    # model dimension from the paper whihc is 512\n",
    "\n",
    "    self.d_model=d_model\n",
    "\n",
    "    self.vocab_size=vocab_size\n",
    "\n",
    "    self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "  def forward(self,x):\n",
    "\n",
    "    # * sqrt(self.d_model) from the research paper\n",
    "\n",
    "    return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b205ff48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:36.975599Z",
     "iopub.status.busy": "2024-11-11T08:18:36.975302Z",
     "iopub.status.idle": "2024-11-11T08:18:36.983442Z",
     "shell.execute_reply": "2024-11-11T08:18:36.982644Z"
    },
    "id": "y0ZRNomBbWlh",
    "papermill": {
     "duration": 0.037049,
     "end_time": "2024-11-11T08:18:36.985478",
     "exception": false,
     "start_time": "2024-11-11T08:18:36.948429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model:int , seq_len:int , dropout:float):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model=d_model\n",
    "\n",
    "    self.seq_len=seq_len\n",
    "\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    pe=torch.zeros(self.seq_len, self.d_model)\n",
    "\n",
    "    # unsequeeze 1 to reshape\n",
    "\n",
    "    positions=torch.arange(0, self.seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    div_term = 10000 ** (torch.arange(0,self.d_model,2) / d_model)\n",
    "\n",
    "    # even poistion encoding\n",
    "\n",
    "    pe[:,0::2]=torch.sin(positions/div_term)\n",
    "\n",
    "    # odd poistion encoding\n",
    "\n",
    "    pe[:,1::2]=torch.cos(positions/div_term)\n",
    "\n",
    "    # for bacth dimensions\n",
    "\n",
    "    pe=pe.unsqueeze(0)\n",
    "\n",
    "    # saving our positional encoding like tunable parameter but it did not update during training\n",
    "\n",
    "    self.register_buffer(\"pe\",pe)\n",
    "\n",
    "  def forward(self,x):\n",
    "\n",
    "    # not want trainable encoding\n",
    "\n",
    "    x=x+ (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n",
    "\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef3d28",
   "metadata": {
    "id": "6SDLFg1JrZ63",
    "papermill": {
     "duration": 0.02587,
     "end_time": "2024-11-11T08:18:37.037606",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.011736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **MultiHead Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da1e32f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.090892Z",
     "iopub.status.busy": "2024-11-11T08:18:37.090584Z",
     "iopub.status.idle": "2024-11-11T08:18:37.103961Z",
     "shell.execute_reply": "2024-11-11T08:18:37.103182Z"
    },
    "id": "iro6I3CdbWo7",
    "papermill": {
     "duration": 0.042135,
     "end_time": "2024-11-11T08:18:37.105807",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.063672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model:int,h:int, dropout:float ):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model=d_model\n",
    "\n",
    "    self.h=h\n",
    "\n",
    "\n",
    "\n",
    "    assert d_model % h==0,\"Dimensions is not divisible by number of heads\"\n",
    "\n",
    "\n",
    "\n",
    "    self.d_k=self.d_model // self.h\n",
    "\n",
    "    #  now query key and value weights\n",
    "\n",
    "    self.w_q=nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.w_k=nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.w_v=nn.Linear(d_model, d_model)\n",
    "\n",
    "    # matrix which we use after concatenating to convert it to same dimensional back\n",
    "\n",
    "    self.w_o=nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "\n",
    "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "\n",
    "    # mask => When we want certain words to NOT interact with others, we \"hide\" them\n",
    "\n",
    "\n",
    "\n",
    "    d_k = query.shape[-1] # The last dimension of query, key, and value\n",
    "\n",
    "\n",
    "\n",
    "    # We calculate the Attention(Q,K,V) as in the formula in the image above\n",
    "\n",
    "    attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n",
    "\n",
    "\n",
    "\n",
    "    # Before applying the softmax, we apply the mask to hide some interactions between words\n",
    "\n",
    "    if mask is not None: # If a mask IS defined...\n",
    "\n",
    "        attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n",
    "\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n",
    "\n",
    "        if dropout is not None: # If a dropout IS defined...\n",
    "\n",
    "            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n",
    "\n",
    "\n",
    "\n",
    "    return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, q, k, v, mask):\n",
    "\n",
    "    query=self.w_q(q)\n",
    "\n",
    "    key=self.w_k(k)\n",
    "\n",
    "    value=self.w_v(v)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # reshaping it for multihead\n",
    "\n",
    "    query=query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "    key=key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "    value=value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "    # Obtaining the output and the attention scores\n",
    "\n",
    "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "\n",
    "\n",
    "    # Obtaining the H matrix\n",
    "\n",
    "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "\n",
    "\n",
    "    return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f9e96",
   "metadata": {
    "id": "9GhKsldVciqm",
    "papermill": {
     "duration": 0.025767,
     "end_time": "2024-11-11T08:18:37.157618",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.131851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Residual Connections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca125772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.210797Z",
     "iopub.status.busy": "2024-11-11T08:18:37.210479Z",
     "iopub.status.idle": "2024-11-11T08:18:37.216004Z",
     "shell.execute_reply": "2024-11-11T08:18:37.215167Z"
    },
    "id": "7HJuYxYvbWsB",
    "papermill": {
     "duration": 0.034366,
     "end_time": "2024-11-11T08:18:37.217941",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.183575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n",
    "\n",
    "        self.norm = LayerNormalization() # We use a normalization layer\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "\n",
    "        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n",
    "\n",
    "        return x + self.dropout(self.norm(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f504b9",
   "metadata": {
    "id": "EtPTepVzc2Ca",
    "papermill": {
     "duration": 0.026097,
     "end_time": "2024-11-11T08:18:37.270295",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.244198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Feed Forward Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62144a",
   "metadata": {
    "id": "5sQhXub4c8lj",
    "papermill": {
     "duration": 0.026078,
     "end_time": "2024-11-11T08:18:37.322699",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.296621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "dff=2048\n",
    "\n",
    "from the paper attention all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509fe23f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.376433Z",
     "iopub.status.busy": "2024-11-11T08:18:37.375916Z",
     "iopub.status.idle": "2024-11-11T08:18:37.382854Z",
     "shell.execute_reply": "2024-11-11T08:18:37.382019Z"
    },
    "id": "a0hRXKjcxo1G",
    "papermill": {
     "duration": 0.035819,
     "end_time": "2024-11-11T08:18:37.384804",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.348985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, eps: float = 10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "\n",
    "        # We define alpha as a trainable parameter and initialize it with ones\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
    "\n",
    "\n",
    "\n",
    "        # We define bias as a trainable parameter and initialize it with zeros\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "\n",
    "        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "\n",
    "\n",
    "\n",
    "        # Returning the normalized input\n",
    "\n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ad2b04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.438359Z",
     "iopub.status.busy": "2024-11-11T08:18:37.438098Z",
     "iopub.status.idle": "2024-11-11T08:18:37.444052Z",
     "shell.execute_reply": "2024-11-11T08:18:37.443230Z"
    },
    "id": "DmWJbtdfbWvK",
    "papermill": {
     "duration": 0.035052,
     "end_time": "2024-11-11T08:18:37.445930",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.410878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # First linear transformation\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout\n",
    "\n",
    "        # Second linear transformation\n",
    "\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ce387",
   "metadata": {
    "id": "d0XbZiz0tyEo",
    "papermill": {
     "duration": 0.025655,
     "end_time": "2024-11-11T08:18:37.498546",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.472891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Encoder Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b9efa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.551918Z",
     "iopub.status.busy": "2024-11-11T08:18:37.551587Z",
     "iopub.status.idle": "2024-11-11T08:18:37.558241Z",
     "shell.execute_reply": "2024-11-11T08:18:37.557438Z"
    },
    "id": "q4rS16YJbWyA",
    "papermill": {
     "duration": 0.035221,
     "end_time": "2024-11-11T08:18:37.559985",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.524764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Storing the self-attention block and feed-forward block\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "\n",
    "        # Applying the first residual connection with the self-attention block\n",
    "\n",
    "        # src_mask for padding tokens\n",
    "\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566c634",
   "metadata": {
    "id": "grN3hgOaEN9j",
    "papermill": {
     "duration": 0.026695,
     "end_time": "2024-11-11T08:18:37.612827",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.586132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Complete Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3771a12f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.666257Z",
     "iopub.status.busy": "2024-11-11T08:18:37.665956Z",
     "iopub.status.idle": "2024-11-11T08:18:37.671682Z",
     "shell.execute_reply": "2024-11-11T08:18:37.670852Z"
    },
    "id": "Kvm3TNeRbW1L",
    "papermill": {
     "duration": 0.03496,
     "end_time": "2024-11-11T08:18:37.673509",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.638549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    # The Encoder takes in instances of 'EncoderBlock'\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers # Storing the EncoderBlocks\n",
    "\n",
    "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # Iterating over each EncoderBlock stored in self.layers\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n",
    "\n",
    "        return self.norm(x) # Normalizing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c986d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.726554Z",
     "iopub.status.busy": "2024-11-11T08:18:37.726261Z",
     "iopub.status.idle": "2024-11-11T08:18:37.733981Z",
     "shell.execute_reply": "2024-11-11T08:18:37.733176Z"
    },
    "id": "GbLC7UuDbW4A",
    "papermill": {
     "duration": 0.036395,
     "end_time": "2024-11-11T08:18:37.735904",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.699509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
    "\n",
    "    def __init__(self,  self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "\n",
    "\n",
    "        # Self-Attention block with query, key, and value plus the target language mask\n",
    "\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "\n",
    "\n",
    "\n",
    "        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n",
    "\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "\n",
    "\n",
    "        # Feed-forward block with residual connections\n",
    "\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22d44777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.789483Z",
     "iopub.status.busy": "2024-11-11T08:18:37.789201Z",
     "iopub.status.idle": "2024-11-11T08:18:37.795033Z",
     "shell.execute_reply": "2024-11-11T08:18:37.794201Z"
    },
    "id": "vuvGHK9_bW7I",
    "papermill": {
     "duration": 0.034556,
     "end_time": "2024-11-11T08:18:37.797007",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.762451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    # The Decoder takes in instances of 'DecoderBlock'\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        self.norm = LayerNormalization() # Layer to normalize the output\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "\n",
    "\n",
    "        # Iterating over each DecoderBlock stored in self.layers\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
    "\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.norm(x) # Returns normalized output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f263b",
   "metadata": {
    "id": "mc8fDvXpOs4b",
    "papermill": {
     "duration": 0.025806,
     "end_time": "2024-11-11T08:18:37.848755",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.822949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Linear Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be80826f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:37.902137Z",
     "iopub.status.busy": "2024-11-11T08:18:37.901847Z",
     "iopub.status.idle": "2024-11-11T08:18:37.907246Z",
     "shell.execute_reply": "2024-11-11T08:18:37.906419Z"
    },
    "id": "kt0uoBlobW9_",
    "papermill": {
     "duration": 0.034032,
     "end_time": "2024-11-11T08:18:37.909068",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.875036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return torch.log_softmax(self.proj(x), dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b84dd",
   "metadata": {
    "id": "MxN_WkkERCj8",
    "papermill": {
     "duration": 0.025877,
     "end_time": "2024-11-11T08:18:37.960912",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.935035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Transformer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88f4e8a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.014064Z",
     "iopub.status.busy": "2024-11-11T08:18:38.013742Z",
     "iopub.status.idle": "2024-11-11T08:18:38.022326Z",
     "shell.execute_reply": "2024-11-11T08:18:38.021681Z"
    },
    "id": "Gvk3CBbVbXA7",
    "papermill": {
     "duration": 0.03737,
     "end_time": "2024-11-11T08:18:38.024196",
     "exception": false,
     "start_time": "2024-11-11T08:18:37.986826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
    "\n",
    "    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: Embedding, tgt_embed: Embedding, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.src_embed = src_embed\n",
    "\n",
    "        self.tgt_embed = tgt_embed\n",
    "\n",
    "        self.src_pos = src_pos\n",
    "\n",
    "        self.tgt_pos = tgt_pos\n",
    "\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "\n",
    "\n",
    "    # Encoder\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "\n",
    "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
    "\n",
    "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
    "\n",
    "        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
    "\n",
    "\n",
    "\n",
    "    # Decoder\n",
    "\n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "\n",
    "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
    "\n",
    "        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "\n",
    "\n",
    "\n",
    "        # Returning the target embeddings, the output of the encoder, and both source and target masks\n",
    "\n",
    "        # The target mask ensures that the model won't 'see' future elements of the sequence\n",
    "\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "\n",
    "\n",
    "    # Applying Projection Layer with the Softmax function to the Decoder output\n",
    "\n",
    "    def project(self, x):\n",
    "\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47fff53d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.077331Z",
     "iopub.status.busy": "2024-11-11T08:18:38.077067Z",
     "iopub.status.idle": "2024-11-11T08:18:38.087712Z",
     "shell.execute_reply": "2024-11-11T08:18:38.086943Z"
    },
    "id": "rYWs_Xj3bXEK",
    "papermill": {
     "duration": 0.039272,
     "end_time": "2024-11-11T08:18:38.089469",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.050197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "\n",
    "\n",
    "\n",
    "    # Creating Embedding layers\n",
    "\n",
    "    src_embed = Embedding(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
    "\n",
    "    tgt_embed = Embedding(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
    "\n",
    "\n",
    "\n",
    "    # Creating Positional Encoding layers\n",
    "\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
    "\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
    "\n",
    "\n",
    "\n",
    "    # Creating EncoderBlocks\n",
    "\n",
    "    encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
    "\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
    "\n",
    "        encoder_self_attention_block = MultiHeadAttention(d_model, h, dropout) # Self-Attention\n",
    "\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "\n",
    "\n",
    "        # Combine layers into an EncoderBlock\n",
    "\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "\n",
    "        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
    "\n",
    "\n",
    "\n",
    "    # Creating DecoderBlocks\n",
    "\n",
    "    decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
    "\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
    "\n",
    "        decoder_self_attention_block = MultiHeadAttention(d_model, h, dropout) # Self-Attention\n",
    "\n",
    "        decoder_cross_attention_block = MultiHeadAttention(d_model, h, dropout) # Cross-Attention\n",
    "\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "\n",
    "\n",
    "        # Combining layers into a DecoderBlock\n",
    "\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "\n",
    "        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
    "\n",
    "\n",
    "\n",
    "    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
    "\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "\n",
    "\n",
    "    # Creating projection layer\n",
    "\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
    "\n",
    "\n",
    "\n",
    "    # Creating the transformer by combining everything above\n",
    "\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize the parameters\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "\n",
    "        if p.dim() > 1:\n",
    "\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "\n",
    "    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca8063ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.142848Z",
     "iopub.status.busy": "2024-11-11T08:18:38.142189Z",
     "iopub.status.idle": "2024-11-11T08:18:38.155404Z",
     "shell.execute_reply": "2024-11-11T08:18:38.154602Z"
    },
    "id": "qhPrx_JzsFD4",
    "papermill": {
     "duration": 0.04187,
     "end_time": "2024-11-11T08:18:38.157281",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.115411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_tokenizer(config, lang):\n",
    "    if lang == config['lang_src']:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    elif lang == config['lang_tgt']:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "\n",
    "    # Adding special tokens\n",
    "    special_tokens = {'additional_special_tokens': ['[SOS]', '[EOS]'], 'pad_token': '[PAD]'}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    print(f\"Tokenizer vocab size after adding special tokens: {tokenizer.vocab_size}\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def casual_mask(size):\n",
    "    \"\"\"Creates a future-masking matrix to prevent decoder from 'seeing' future tokens.\"\"\"\n",
    "    return torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool)\n",
    "\n",
    "\n",
    "def train_model(config, df):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, df)\n",
    "\n",
    "    model = get_model(config, tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n",
    "    model.src_embed.embedding = nn.Embedding(tokenizer_src.vocab_size, config['d_model'])\n",
    "    model.tgt_embed.embedding = nn.Embedding(tokenizer_tgt.vocab_size, config['d_model'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.pad_token_id, label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            encoder_input, decoder_input = batch['encoder_input'].to(device), batch['decoder_input'].to(device)\n",
    "            encoder_mask, decoder_mask = batch['encoder_mask'].to(device), batch['decoder_mask'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.vocab_size), label.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c097d1a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.210512Z",
     "iopub.status.busy": "2024-11-11T08:18:38.210233Z",
     "iopub.status.idle": "2024-11-11T08:18:38.215119Z",
     "shell.execute_reply": "2024-11-11T08:18:38.214302Z"
    },
    "id": "Oruq6Y7JEzWl",
    "papermill": {
     "duration": 0.033436,
     "end_time": "2024-11-11T08:18:38.216907",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.183471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def build_tokenizer(config, lang):\n",
    "    if lang == config['lang_src']:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # English tokenizer\n",
    "    elif lang == config['lang_tgt']:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")  # German tokenizer\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "    return tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "632c9c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.269998Z",
     "iopub.status.busy": "2024-11-11T08:18:38.269721Z",
     "iopub.status.idle": "2024-11-11T08:18:38.274367Z",
     "shell.execute_reply": "2024-11-11T08:18:38.273659Z"
    },
    "id": "6DJvrTH7FFEn",
    "papermill": {
     "duration": 0.03338,
     "end_time": "2024-11-11T08:18:38.276203",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.242823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def casual_mask(size):\n",
    "\n",
    "#         # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "\n",
    "#         mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "\n",
    "#         return mask == 0\n",
    "\n",
    "def casual_mask(size):\n",
    "    \"\"\"Creates a causal mask of shape (1, size, size) to prevent attention to future tokens.\"\"\"\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.bool)\n",
    "    return mask == 0  # Invert the mask so that future positions are masked (set to False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ffdd68d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.329536Z",
     "iopub.status.busy": "2024-11-11T08:18:38.329273Z",
     "iopub.status.idle": "2024-11-11T08:18:38.340257Z",
     "shell.execute_reply": "2024-11-11T08:18:38.339470Z"
    },
    "id": "DPrhyf-1Ezct",
    "papermill": {
     "duration": 0.039978,
     "end_time": "2024-11-11T08:18:38.342124",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.302146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        self.df = df\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "        self.sos_token_id = tokenizer_tgt.convert_tokens_to_ids(\"[SOS]\")\n",
    "        self.eos_token_id = tokenizer_tgt.convert_tokens_to_ids(\"[EOS]\")\n",
    "        self.pad_token_id = tokenizer_tgt.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_text = self.df.iloc[index][self.src_lang]\n",
    "        tgt_text = self.df.iloc[index][self.tgt_lang]\n",
    "\n",
    "        # Tokenize source and target texts\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text, truncation=True, padding=\"max_length\", max_length=self.seq_len)\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text, truncation=True, padding=\"max_length\", max_length=self.seq_len)\n",
    "\n",
    "        # Convert to tensors\n",
    "        encoder_input = torch.tensor(enc_input_tokens, dtype=torch.long)\n",
    "        decoder_input = torch.tensor([self.sos_token_id] + dec_input_tokens[:-1], dtype=torch.long)  # Add [SOS] at the beginning\n",
    "        label = torch.tensor(dec_input_tokens, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class BilingualDataset(Dataset):\n",
    "#     def __init__(self, df, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "#         self.df = df\n",
    "#         self.tokenizer_src = tokenizer_src\n",
    "#         self.tokenizer_tgt = tokenizer_tgt\n",
    "#         self.src_lang = src_lang\n",
    "#         self.tgt_lang = tgt_lang\n",
    "#         self.seq_len = seq_len\n",
    "#         self.sos_token_id = self.tokenizer_tgt.convert_tokens_to_ids(\"[SOS]\")\n",
    "#         self.eos_token_id = self.tokenizer_tgt.convert_tokens_to_ids(\"[EOS]\")\n",
    "#         self.pad_token_id = self.tokenizer_tgt.pad_token_id\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         src_text = self.df.iloc[index][self.src_lang]\n",
    "#         tgt_text = self.df.iloc[index][self.tgt_lang]\n",
    "\n",
    "#         # Tokenize source and target texts\n",
    "#         enc_input_tokens = self.tokenizer_src.encode(src_text, truncation=True, padding=\"max_length\", max_length=self.seq_len)\n",
    "#         dec_input_tokens = self.tokenizer_tgt.encode(tgt_text, truncation=True, padding=\"max_length\", max_length=self.seq_len)\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         encoder_input = torch.tensor(enc_input_tokens, dtype=torch.long)\n",
    "#         decoder_input = torch.tensor([self.sos_token_id] + dec_input_tokens[:-1], dtype=torch.long)  # Add [SOS] at the beginning\n",
    "#         label = torch.tensor(dec_input_tokens + [self.eos_token_id], dtype=torch.long)  # Add [EOS] at the end\n",
    "\n",
    "#         return {\n",
    "#             'encoder_input': encoder_input,\n",
    "#             'decoder_input': decoder_input,\n",
    "#             'encoder_mask': (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n",
    "#             'decoder_mask': (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n",
    "#             'label': label,\n",
    "#             'src_text': src_text,\n",
    "#             'tgt_text': tgt_text\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89e86c3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.397545Z",
     "iopub.status.busy": "2024-11-11T08:18:38.397245Z",
     "iopub.status.idle": "2024-11-11T08:18:38.404010Z",
     "shell.execute_reply": "2024-11-11T08:18:38.403139Z"
    },
    "id": "WNZ0IuyvEzfD",
    "papermill": {
     "duration": 0.037805,
     "end_time": "2024-11-11T08:18:38.406106",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.368301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_dataloaders(config, df):\n",
    "    tokenizer_src = build_tokenizer(config, config['lang_src'])\n",
    "    tokenizer_tgt = build_tokenizer(config, config['lang_tgt'])\n",
    "    train_size = int(0.9 * len(df))\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:]\n",
    "\n",
    "    train_ds = BilingualDataset(train_df, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_df, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader, tokenizer_src, tokenizer_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f37b40b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.463688Z",
     "iopub.status.busy": "2024-11-11T08:18:38.463356Z",
     "iopub.status.idle": "2024-11-11T08:18:38.468760Z",
     "shell.execute_reply": "2024-11-11T08:18:38.467773Z"
    },
    "id": "H7DWG-4XEzho",
    "papermill": {
     "duration": 0.036549,
     "end_time": "2024-11-11T08:18:38.470768",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.434219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "    model.resize_token_embeddings(len(tokenizer_src))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ede293",
   "metadata": {
    "id": "EPi2acy2Ezj3",
    "papermill": {
     "duration": 0.025856,
     "end_time": "2024-11-11T08:18:38.524784",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.498928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1c0aec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:38.578594Z",
     "iopub.status.busy": "2024-11-11T08:18:38.578298Z",
     "iopub.status.idle": "2024-11-11T08:18:46.486895Z",
     "shell.execute_reply": "2024-11-11T08:18:46.485892Z"
    },
    "id": "Q0BDUcQtsFKP",
    "papermill": {
     "duration": 7.938374,
     "end_time": "2024-11-11T08:18:46.489178",
     "exception": false,
     "start_time": "2024-11-11T08:18:38.550804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_config():\n",
    "\n",
    "    return{\n",
    "\n",
    "        'batch_size': 6,\n",
    "\n",
    "        'num_epochs': 20,\n",
    "\n",
    "        'lr': 10**-4,\n",
    "\n",
    "        'seq_len': 350,\n",
    "\n",
    "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
    "\n",
    "        'lang_src': 'en',\n",
    "\n",
    "        'lang_tgt': 'de',\n",
    "\n",
    "        'model_folder': '/kaggle/working/weights',\n",
    "\n",
    "        'model_basename': '/kaggle/working/tmodel_',\n",
    "\n",
    "        'preload': None,\n",
    "\n",
    "        'tokenizer_file': 'tokenizer_{0}.json',\n",
    "\n",
    "        'experiment_name': 'runs/tmodel'\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to construct the path for saving and retrieving model weights\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "\n",
    "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "\n",
    "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "\n",
    "    return str(Path('.')/ model_folder/ model_filename)\n",
    "\n",
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.convert_tokens_to_ids('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.convert_tokens_to_ids('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    decoder_input = torch.full((1, 1), sos_idx, dtype=torch.long).to(device)\n",
    "\n",
    "    while True:\n",
    "        if decoder_input.size(1) >= max_len:\n",
    "            break\n",
    "\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).to(device)\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "        prob = model.project(out[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, next_word.unsqueeze(0)], dim=1)\n",
    "\n",
    "        if next_word.item() == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "\n",
    "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "\n",
    "\n",
    "\n",
    "    console_width = 80 # Fixed witdh for printed messages\n",
    "\n",
    "\n",
    "\n",
    "    # Creating evaluation loop\n",
    "\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "\n",
    "        for batch in validation_ds:\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "\n",
    "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "\n",
    "\n",
    "\n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "\n",
    "\n",
    "            # Retrieving source and target texts from the batch\n",
    "\n",
    "            source_text = batch['src_text'][0]\n",
    "\n",
    "            target_text = batch['tgt_text'][0] # True translation\n",
    "\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "\n",
    "\n",
    "            # Printing results\n",
    "\n",
    "            print_msg('-'*console_width)\n",
    "\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "\n",
    "\n",
    "\n",
    "            # After two examples, we break the loop\n",
    "\n",
    "            if count == num_examples:\n",
    "\n",
    "                break\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "file_path = '/kaggle/input/less-train/less-train.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df=df.iloc[:100000,:]\n",
    "\n",
    "# Drop rows with NaN in 'en' or 'de' columns\n",
    "\n",
    "df = df.dropna(subset=['en', 'de'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b5a16a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:46.543959Z",
     "iopub.status.busy": "2024-11-11T08:18:46.543160Z",
     "iopub.status.idle": "2024-11-11T08:18:46.548153Z",
     "shell.execute_reply": "2024-11-11T08:18:46.547289Z"
    },
    "id": "xLGLQ5r9sFN6",
    "papermill": {
     "duration": 0.034126,
     "end_time": "2024-11-11T08:18:46.550097",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.515971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "\n",
    "\n",
    "\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fc37695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:46.603790Z",
     "iopub.status.busy": "2024-11-11T08:18:46.603482Z",
     "iopub.status.idle": "2024-11-11T08:18:46.620563Z",
     "shell.execute_reply": "2024-11-11T08:18:46.619743Z"
    },
    "id": "Wi2V_eh6sFRQ",
    "papermill": {
     "duration": 0.045946,
     "end_time": "2024-11-11T08:18:46.622418",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.576472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_ds(config, df):\n",
    "    # Get dataloaders and tokenizers for source and target languages\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataloaders(config, df)\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "# Training function that now accepts `df` for training data\n",
    "def train_model(config, df):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    # Creating model directory to store weights\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Retrieving dataloaders and tokenizers for source and target languages\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, df)\n",
    "\n",
    "    # Initializing model on the GPU\n",
    "    model = get_model(config, tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n",
    "\n",
    "    # Tensorboard setup\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    # Setting up the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # Initializing epoch and global step variables\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Checking if there is a pre-trained model to load\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "\n",
    "        # Restore saved states\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "\n",
    "    # Setting up CrossEntropyLoss with label smoothing and padding token ignore\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.convert_tokens_to_ids('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
    "        for batch in batch_iterator:\n",
    "            model.train()\n",
    "\n",
    "            # Move data to GPU\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.vocab_size), label.view(-1))\n",
    "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of each epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device,\n",
    "                       lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "        print(f\"Checkpoint saved at {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c3b96",
   "metadata": {
    "papermill": {
     "duration": 0.025705,
     "end_time": "2024-11-11T08:18:46.674834",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.649129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a28250b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:46.727898Z",
     "iopub.status.busy": "2024-11-11T08:18:46.727581Z",
     "iopub.status.idle": "2024-11-11T08:18:46.731558Z",
     "shell.execute_reply": "2024-11-11T08:18:46.730744Z"
    },
    "id": "9pVM43RjsFUa",
    "papermill": {
     "duration": 0.032629,
     "end_time": "2024-11-11T08:18:46.733441",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.700812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore') # Filtering warnings\n",
    "config = get_config() # Retrieving config settings\n",
    "# train_model(config, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc5959",
   "metadata": {
    "id": "pUJRk073sFX3",
    "papermill": {
     "duration": 0.025797,
     "end_time": "2024-11-11T08:18:46.785266",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.759469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a12621d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:46.838385Z",
     "iopub.status.busy": "2024-11-11T08:18:46.838071Z",
     "iopub.status.idle": "2024-11-11T08:18:46.842243Z",
     "shell.execute_reply": "2024-11-11T08:18:46.841364Z"
    },
    "id": "X9N1lX3nsFbO",
    "papermill": {
     "duration": 0.033137,
     "end_time": "2024-11-11T08:18:46.844277",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.811140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# writer = SummaryWriter(config['experiment_name'])\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# # model_filename = get_weights_file_path(config, config['preload'])\n",
    "# state = torch.load(\"tmodel_00.pt\")\n",
    "# global_step = state['global_step']\n",
    "# train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, df)\n",
    "# model = get_model(config, tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n",
    "# print()\n",
    "# run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device,\n",
    "#                        lambda msg: batch_iterator.write(msg), global_step, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6a7de",
   "metadata": {
    "id": "U8DwQqZssFeX",
    "papermill": {
     "duration": 0.026032,
     "end_time": "2024-11-11T08:18:46.896854",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.870822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5452877",
   "metadata": {
    "id": "D7NTkoP_sFhl",
    "papermill": {
     "duration": 0.026066,
     "end_time": "2024-11-11T08:18:46.948844",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.922778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5761b23",
   "metadata": {
    "id": "2Eg2hXc0sFk_",
    "papermill": {
     "duration": 0.025794,
     "end_time": "2024-11-11T08:18:47.000539",
     "exception": false,
     "start_time": "2024-11-11T08:18:46.974745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916e72e",
   "metadata": {
    "id": "KKm_MCaasFoU",
    "papermill": {
     "duration": 0.025742,
     "end_time": "2024-11-11T08:18:47.053001",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.027259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03ad66",
   "metadata": {
    "id": "rgkqFSjPsFsd",
    "papermill": {
     "duration": 0.025699,
     "end_time": "2024-11-11T08:18:47.104528",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.078829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458dd4bd",
   "metadata": {
    "id": "_KJcwuzksFwj",
    "papermill": {
     "duration": 0.025796,
     "end_time": "2024-11-11T08:18:47.156270",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.130474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96c38a6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:47.210215Z",
     "iopub.status.busy": "2024-11-11T08:18:47.209861Z",
     "iopub.status.idle": "2024-11-11T08:18:47.230380Z",
     "shell.execute_reply": "2024-11-11T08:18:47.229511Z"
    },
    "id": "TLimqv3molVj",
    "papermill": {
     "duration": 0.050238,
     "end_time": "2024-11-11T08:18:47.232415",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.182177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Tokenizer function modified for DataFrame usage\n",
    "\n",
    "# def build_tokenizer(config, df, lang):\n",
    "\n",
    "#     tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "\n",
    "\n",
    "\n",
    "#     if not Path.exists(tokenizer_path):\n",
    "\n",
    "#         tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "\n",
    "#         tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "\n",
    "\n",
    "#         # Training the tokenizer on sentences from the DataFrame\n",
    "\n",
    "#         trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "\n",
    "#         tokenizer.train_from_iterator(df[lang].tolist(), trainer=trainer)\n",
    "\n",
    "#         tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "#     return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_ds(config, df):\n",
    "\n",
    "#     # Using the DataFrame `df` instead of loading from Hugging Face\n",
    "\n",
    "#     tokenizer_src = build_tokenizer(config, df, config['lang_src'])\n",
    "\n",
    "#     tokenizer_tgt = build_tokenizer(config, df, config['lang_tgt'])\n",
    "\n",
    "\n",
    "\n",
    "#     # Splitting the dataset for training and validation\n",
    "\n",
    "#     train_ds_size = int(0.9 * len(df))\n",
    "\n",
    "#     train_df = df[:train_ds_size]\n",
    "\n",
    "#     val_df = df[train_ds_size:]\n",
    "\n",
    "\n",
    "\n",
    "#     # Creating dataset instances for training and validation\n",
    "\n",
    "#     train_ds = BilingualDataset(train_df, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "#     val_ds = BilingualDataset(val_df, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "\n",
    "\n",
    "#     # Dataloaders\n",
    "\n",
    "#     train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "#     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "#     return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class BilingualDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, df, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.seq_len = seq_len\n",
    "\n",
    "#         self.df = df\n",
    "\n",
    "#         self.tokenizer_src = tokenizer_src\n",
    "\n",
    "#         self.tokenizer_tgt = tokenizer_tgt\n",
    "\n",
    "#         self.src_lang = src_lang\n",
    "\n",
    "#         self.tgt_lang = tgt_lang\n",
    "\n",
    "#         self.sos_token = torch.tensor([tokenizer_tgt.convert_tokens_to_ids(\"[SOS]\")], dtype=torch.int64)\n",
    "\n",
    "#         self.eos_token = torch.tensor([tokenizer_tgt.convert_tokens_to_ids(\"[EOS]\")], dtype=torch.int64)\n",
    "\n",
    "#         self.pad_token = torch.tensor([tokenizer_tgt.convert_tokens_to_ids(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "\n",
    "#         return len(self.df)\n",
    "\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index: Any) -> Any:\n",
    "\n",
    "#         src_text = self.df.iloc[index][self.src_lang]\n",
    "\n",
    "#         tgt_text = self.df.iloc[index][self.tgt_lang]\n",
    "\n",
    "\n",
    "\n",
    "#         # Tokenizing\n",
    "\n",
    "#         enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "\n",
    "#         dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "\n",
    "\n",
    "#         enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
    "\n",
    "#         dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "\n",
    "\n",
    "#         if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "\n",
    "#             raise ValueError('Sentence is too long')\n",
    "\n",
    "\n",
    "\n",
    "#         encoder_input = torch.cat(\n",
    "\n",
    "#             [self.sos_token, torch.tensor(enc_input_tokens, dtype=torch.int64), self.eos_token,\n",
    "\n",
    "#              torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)]\n",
    "\n",
    "#         )\n",
    "\n",
    "#         decoder_input = torch.cat(\n",
    "\n",
    "#             [self.sos_token, torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "\n",
    "#              torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)]\n",
    "\n",
    "#         )\n",
    "\n",
    "#         label = torch.cat(\n",
    "\n",
    "#             [torch.tensor(dec_input_tokens, dtype=torch.int64), self.eos_token,\n",
    "\n",
    "#              torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)]\n",
    "\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "#         assert encoder_input.size(0) == self.seq_len\n",
    "\n",
    "#         assert decoder_input.size(0) == self.seq_len\n",
    "\n",
    "#         assert label.size(0) == self.seq_len\n",
    "\n",
    "\n",
    "\n",
    "#         return {\n",
    "\n",
    "#             'encoder_input': encoder_input,\n",
    "\n",
    "#             'decoder_input': decoder_input,\n",
    "\n",
    "#             'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "\n",
    "#             'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
    "\n",
    "#             'label': label,\n",
    "\n",
    "#             'src_text': src_text,\n",
    "\n",
    "#             'tgt_text': tgt_text\n",
    "\n",
    "#         }\n",
    "\n",
    "\n",
    "\n",
    "# # Now pass `df` as an argument to `get_ds` in the `train_model` function:\n",
    "\n",
    "# train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, df)\n",
    "\n",
    "\n",
    "\n",
    "# # Iterating through dataset to extract the original sentence and its translation\n",
    "\n",
    "# def get_all_sentences(ds, lang):\n",
    "\n",
    "#     for pair in ds:\n",
    "\n",
    "#         yield pair['translation'][lang]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def casual_mask(size):\n",
    "\n",
    "#         # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "\n",
    "#         mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "\n",
    "#         return mask == 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "\n",
    "#     # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "\n",
    "#     sos_idx = tokenizer_tgt.convert_tokens_to_ids('[SOS]')\n",
    "\n",
    "#     eos_idx = tokenizer_tgt.convert_tokens_to_ids('[EOS]')\n",
    "\n",
    "\n",
    "\n",
    "#     # Computing the output of the encoder for the source sequence\n",
    "\n",
    "#     encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "#     # Initializing the decoder input with the Start of Sentence token\n",
    "\n",
    "#     decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#     # Looping until the 'max_len', maximum length, is reached\n",
    "\n",
    "#     while True:\n",
    "\n",
    "#         if decoder_input.size(1) == max_len:\n",
    "\n",
    "#             break\n",
    "\n",
    "\n",
    "\n",
    "#         # Building a mask for the decoder input\n",
    "\n",
    "#         decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#         # Calculating the output of the decoder\n",
    "\n",
    "#         out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "\n",
    "\n",
    "#         # Applying the projection layer to get the probabilities for the next token\n",
    "\n",
    "#         prob = model.project(out[:, -1])\n",
    "\n",
    "\n",
    "\n",
    "#         # Selecting token with the highest probability\n",
    "\n",
    "#         _, next_word = torch.max(prob, dim=1)\n",
    "\n",
    "#         decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "#         # If the next token is an End of Sentence token, we finish the loop\n",
    "\n",
    "#         if next_word == eos_idx:\n",
    "\n",
    "#             break\n",
    "\n",
    "\n",
    "\n",
    "#     return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "\n",
    "#     model.eval() # Setting model to evaluation mode\n",
    "\n",
    "#     count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "\n",
    "\n",
    "\n",
    "#     console_width = 80 # Fixed witdh for printed messages\n",
    "\n",
    "\n",
    "\n",
    "#     # Creating evaluation loop\n",
    "\n",
    "#     with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "\n",
    "#         for batch in validation_ds:\n",
    "\n",
    "#             count += 1\n",
    "\n",
    "#             encoder_input = batch['encoder_input'].to(device)\n",
    "\n",
    "#             encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "#             # Ensuring that the batch_size of the validation set is 1\n",
    "\n",
    "#             assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "\n",
    "\n",
    "\n",
    "#             # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "\n",
    "#             model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "\n",
    "\n",
    "#             # Retrieving source and target texts from the batch\n",
    "\n",
    "#             source_text = batch['src_text'][0]\n",
    "\n",
    "#             target_text = batch['tgt_text'][0] # True translation\n",
    "\n",
    "#             model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "\n",
    "\n",
    "#             # Printing results\n",
    "\n",
    "#             print_msg('-'*console_width)\n",
    "\n",
    "#             print_msg(f'SOURCE: {source_text}')\n",
    "\n",
    "#             print_msg(f'TARGET: {target_text}')\n",
    "\n",
    "#             print_msg(f'PREDICTED: {model_out_text}')\n",
    "\n",
    "\n",
    "\n",
    "#             # After two examples, we break the loop\n",
    "\n",
    "#             if count == num_examples:\n",
    "\n",
    "#                 break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
    "\n",
    "# def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "\n",
    "\n",
    "\n",
    "#     # Loading model using the 'build_transformer' function.\n",
    "\n",
    "#     # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "\n",
    "#     model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_config():\n",
    "\n",
    "#     return{\n",
    "\n",
    "#         'batch_size': 8,\n",
    "\n",
    "#         'num_epochs': 20,\n",
    "\n",
    "#         'lr': 10**-4,\n",
    "\n",
    "#         'seq_len': 350,\n",
    "\n",
    "#         'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
    "\n",
    "#         'lang_src': 'en',\n",
    "\n",
    "#         'lang_tgt': 'it',\n",
    "\n",
    "#         'model_folder': 'weights',\n",
    "\n",
    "#         'model_basename': 'tmodel_',\n",
    "\n",
    "#         'preload': None,\n",
    "\n",
    "#         'tokenizer_file': 'tokenizer_{0}.json',\n",
    "\n",
    "#         'experiment_name': 'runs/tmodel'\n",
    "\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Function to construct the path for saving and retrieving model weights\n",
    "\n",
    "# def get_weights_file_path(config, epoch: str):\n",
    "\n",
    "#     model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "\n",
    "#     model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "\n",
    "#     model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "\n",
    "#     return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def train_model(config):\n",
    "\n",
    "#     # Setting up device to run on GPU to train faster\n",
    "\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     print(f\"Using device {device}\")\n",
    "\n",
    "\n",
    "\n",
    "#     # Creating model directory to store weights\n",
    "\n",
    "#     Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "#     # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "\n",
    "#     train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "\n",
    "\n",
    "\n",
    "#     # Initializing model on the GPU using the 'get_model' function\n",
    "\n",
    "#     model = get_model(config,tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#     # Tensorboard\n",
    "\n",
    "#     writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "\n",
    "\n",
    "#     # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "\n",
    "#     # config' dictionary plus an epsilon value\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "#     # Initializing epoch and global step variables\n",
    "\n",
    "#     initial_epoch = 0\n",
    "\n",
    "#     global_step = 0\n",
    "\n",
    "\n",
    "\n",
    "#     # Checking if there is a pre-trained model to load\n",
    "\n",
    "#     # If true, loads it\n",
    "\n",
    "#     if config['preload']:\n",
    "\n",
    "#         model_filename = get_weights_file_path(config, config['preload'])\n",
    "\n",
    "#         print(f'Preloading model {model_filename}')\n",
    "\n",
    "#         state = torch.load(model_filename) # Loading model\n",
    "\n",
    "\n",
    "\n",
    "#         # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "\n",
    "#         initial_epoch = state['epoch'] + 1\n",
    "\n",
    "#         # Loading the optimizer state from the saved model\n",
    "\n",
    "#         optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "\n",
    "#         # Loading the global step state from the saved model\n",
    "\n",
    "#         global_step = state['global_step']\n",
    "\n",
    "\n",
    "\n",
    "#     # Initializing CrossEntropyLoss function for training\n",
    "\n",
    "#     # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "\n",
    "#     # We also apply label_smoothing to prevent overfitting\n",
    "\n",
    "#     loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.convert_tokens_to_ids('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#     # Initializing training loop\n",
    "\n",
    "\n",
    "\n",
    "#     # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "\n",
    "#     # the number of epochs informed in the config\n",
    "\n",
    "#     for epoch in range(initial_epoch, config['num_epochs']):\n",
    "\n",
    "\n",
    "\n",
    "#         # Initializing an iterator over the training dataloader\n",
    "\n",
    "#         # We also use tqdm to display a progress bar\n",
    "\n",
    "#         batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
    "\n",
    "\n",
    "\n",
    "#         # For each batch...\n",
    "\n",
    "#         for batch in batch_iterator:\n",
    "\n",
    "#             model.train() # Train the model\n",
    "\n",
    "\n",
    "\n",
    "#             # Loading input data and masks onto the GPU\n",
    "\n",
    "#             encoder_input = batch['encoder_input'].to(device)\n",
    "\n",
    "#             decoder_input = batch['decoder_input'].to(device)\n",
    "\n",
    "#             encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "#             decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "#             # Running tensors through the Transformer\n",
    "\n",
    "#             encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "\n",
    "#             decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "\n",
    "#             proj_output = model.project(decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "#             # Loading the target labels onto the GPU\n",
    "\n",
    "#             label = batch['label'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "#             # Computing loss between model's output and true labels\n",
    "\n",
    "#             loss = loss_fn(proj_output.view(-1, tokenizer_tgt.vocab_size), label.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "#             # Updating progress bar\n",
    "\n",
    "#             batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "\n",
    "\n",
    "#             writer.add_scalar('train loss', loss.item(), global_step)\n",
    "\n",
    "#             writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "#             # Performing backpropagation\n",
    "\n",
    "#             loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "#             # Updating parameters based on the gradients\n",
    "\n",
    "#             optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "#             # Clearing the gradients to prepare for the next batch\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "#             global_step += 1 # Updating global step count\n",
    "\n",
    "\n",
    "\n",
    "#         # We run the 'run_validation' function at the end of each epoch\n",
    "\n",
    "#         # to evaluate model performance\n",
    "\n",
    "#         run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "\n",
    "\n",
    "#         # Saving model\n",
    "\n",
    "#         model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "\n",
    "#         # Writting current model state to the 'model_filename'\n",
    "\n",
    "#         torch.save({\n",
    "\n",
    "#             'epoch': epoch, # Current epoch\n",
    "\n",
    "#             'model_state_dict': model.state_dict(),# Current model state\n",
    "\n",
    "#             'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "\n",
    "#             'global_step': global_step # Current global step\n",
    "\n",
    "#         }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d951d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:47.285575Z",
     "iopub.status.busy": "2024-11-11T08:18:47.285296Z",
     "iopub.status.idle": "2024-11-11T08:18:47.288990Z",
     "shell.execute_reply": "2024-11-11T08:18:47.288140Z"
    },
    "id": "-yrLeWgjolYn",
    "papermill": {
     "duration": 0.032418,
     "end_time": "2024-11-11T08:18:47.290776",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.258358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     warnings.filterwarnings('ignore') # Filtering warnings\n",
    "\n",
    "#     config = get_config() # Retrieving config settings\n",
    "\n",
    "#     train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6212b",
   "metadata": {
    "id": "6jUHjIl9olbk",
    "papermill": {
     "duration": 0.02604,
     "end_time": "2024-11-11T08:18:47.342934",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.316894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458dcfcc",
   "metadata": {
    "id": "t2qpgnKOolem",
    "papermill": {
     "duration": 0.025798,
     "end_time": "2024-11-11T08:18:47.394970",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.369172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bce806",
   "metadata": {
    "id": "64UKw3VRolhn",
    "papermill": {
     "duration": 0.025872,
     "end_time": "2024-11-11T08:18:47.447077",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.421205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5bd175",
   "metadata": {
    "id": "YWyzRyU7olk6",
    "papermill": {
     "duration": 0.026047,
     "end_time": "2024-11-11T08:18:47.499114",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.473067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f740b8",
   "metadata": {
    "id": "-oMKykC7olnw",
    "papermill": {
     "duration": 0.026536,
     "end_time": "2024-11-11T08:18:47.552306",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.525770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcafe6c",
   "metadata": {
    "id": "bZjcMFcNolqk",
    "papermill": {
     "duration": 0.026844,
     "end_time": "2024-11-11T08:18:47.606662",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.579818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfb4e0",
   "metadata": {
    "id": "HaRFmeoiolto",
    "papermill": {
     "duration": 0.025749,
     "end_time": "2024-11-11T08:18:47.659961",
     "exception": false,
     "start_time": "2024-11-11T08:18:47.634212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6049711,
     "sourceId": 9858054,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 131.803494,
   "end_time": "2024-11-11T08:18:50.028546",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-11T08:16:38.225052",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
