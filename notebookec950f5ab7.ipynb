{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9858054,"sourceType":"datasetVersion","datasetId":6049711}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nimport math\n\nimport warnings\n\nfrom tqdm import tqdm\n\nfrom typing import Any\n\n\n\n\n\n\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\nfrom tqdm import tqdm\n\nfrom pathlib import Path\n\nfrom tokenizers import Tokenizer\n\nfrom tokenizers.models import WordLevel\n\nfrom tokenizers.trainers import WordLevelTrainer\n\nfrom tokenizers.pre_tokenizers import Whitespace\n\nimport os\n\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport transformers\n\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"id":"Ge1CkjEpX5yI","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:01.919535Z","iopub.execute_input":"2024-11-12T07:33:01.919868Z","iopub.status.idle":"2024-11-12T07:33:01.928161Z","shell.execute_reply.started":"2024-11-12T07:33:01.919806Z","shell.execute_reply":"2024-11-12T07:33:01.927174Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!conda install -y gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T02:50:50.715907Z","iopub.execute_input":"2024-11-12T02:50:50.716215Z","iopub.status.idle":"2024-11-12T02:52:21.148254Z","shell.execute_reply.started":"2024-11-12T02:50:50.716183Z","shell.execute_reply":"2024-11-12T02:52:21.147265Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - rapidsai\n - nvidia\n - nodefaults\n - conda-forge\n - defaults\n - pytorch\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    conda-24.9.2               |  py310hff52083_0         895 KB  conda-forge\n    filelock-3.16.1            |     pyhd8ed1ab_0          17 KB  conda-forge\n    gdown-5.2.0                |     pyhd8ed1ab_0          21 KB  conda-forge\n    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         3.7 MB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.16.1-pyhd8ed1ab_0 \n  gdown              conda-forge/noarch::gdown-5.2.0-pyhd8ed1ab_0 \n\nThe following packages will be UPDATED:\n\n  conda                              24.9.0-py310hff52083_0 --> 24.9.2-py310hff52083_0 \n  openssl                                  3.3.2-hb9d3cd8_0 --> 3.4.0-hb9d3cd8_0 \n\n\n\nDownloading and Extracting Packages:\nopenssl-3.4.0        | 2.8 MB    |                                       |   0% \nconda-24.9.2         | 895 KB    |                                       |   0% \u001b[A\n\ngdown-5.2.0          | 21 KB     |                                       |   0% \u001b[A\u001b[A\n\n\nopenssl-3.4.0        | 2.8 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\nconda-24.9.2         | 895 KB    | ##6                                   |   7% \u001b[A\n\n\nfilelock-3.16.1      | 17 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\nfilelock-3.16.1      | 17 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\ngdown-5.2.0          | 21 KB     | ##################################### | 100% \u001b[A\u001b[A\n\nopenssl-3.4.0        | 2.8 MB    | ##################################### | 100% \u001b[A\u001b[A\nconda-24.9.2         | 895 KB    | ##################################### | 100% \u001b[A\n                                                                                \u001b[A\n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T02:52:21.150083Z","iopub.execute_input":"2024-11-12T02:52:21.150416Z","iopub.status.idle":"2024-11-12T02:52:33.577780Z","shell.execute_reply.started":"2024-11-12T02:52:21.150380Z","shell.execute_reply":"2024-11-12T02:52:33.576822Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Enable parallelism in tokenization\n# transformers.utils.logging.enable_propagation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:10.111313Z","iopub.execute_input":"2024-11-12T07:33:10.111791Z","iopub.status.idle":"2024-11-12T07:33:10.115961Z","shell.execute_reply.started":"2024-11-12T07:33:10.111753Z","shell.execute_reply":"2024-11-12T07:33:10.114971Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Embedding**","metadata":{"id":"s9EjH3LtYJXR"}},{"cell_type":"code","source":"class Embedding(nn.Module):\n\n  def __init__(self, d_model:int, vocab_size:int):\n\n    super().__init__()\n\n    # model dimension from the paper whihc is 512\n\n    self.d_model=d_model\n\n    self.vocab_size=vocab_size\n\n    self.embedding=nn.Embedding(vocab_size,d_model)\n\n  def forward(self,x):\n\n    # * sqrt(self.d_model) from the research paper\n\n    return self.embedding(x) * math.sqrt(self.d_model)","metadata":{"id":"vHaeRuqhX9I_","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:10.615763Z","iopub.execute_input":"2024-11-12T07:33:10.616145Z","iopub.status.idle":"2024-11-12T07:33:10.622138Z","shell.execute_reply.started":"2024-11-12T07:33:10.616108Z","shell.execute_reply":"2024-11-12T07:33:10.621132Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n  def __init__(self, d_model:int , seq_len:int , dropout:float):\n\n    super().__init__()\n\n    self.d_model=d_model\n\n    self.seq_len=seq_len\n\n    self.dropout=nn.Dropout(dropout)\n\n\n\n    pe=torch.zeros(self.seq_len, self.d_model)\n\n    # unsequeeze 1 to reshape\n\n    positions=torch.arange(0, self.seq_len, dtype=torch.float).unsqueeze(1)\n\n    div_term = 10000 ** (torch.arange(0,self.d_model,2) / d_model)\n\n    # even poistion encoding\n\n    pe[:,0::2]=torch.sin(positions/div_term)\n\n    # odd poistion encoding\n\n    pe[:,1::2]=torch.cos(positions/div_term)\n\n    # for bacth dimensions\n\n    pe=pe.unsqueeze(0)\n\n    # saving our positional encoding like tunable parameter but it did not update during training\n\n    self.register_buffer(\"pe\",pe)\n\n  def forward(self,x):\n\n    # not want trainable encoding\n\n    x=x+ (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n\n    return self.dropout(x)","metadata":{"id":"y0ZRNomBbWlh","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:10.784904Z","iopub.execute_input":"2024-11-12T07:33:10.785218Z","iopub.status.idle":"2024-11-12T07:33:10.794134Z","shell.execute_reply.started":"2024-11-12T07:33:10.785185Z","shell.execute_reply":"2024-11-12T07:33:10.793122Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# **MultiHead Attention**","metadata":{"id":"6SDLFg1JrZ63"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n  def __init__(self, d_model:int,h:int, dropout:float ):\n\n    super().__init__()\n\n    self.d_model=d_model\n\n    self.h=h\n\n\n\n    assert d_model % h==0,\"Dimensions is not divisible by number of heads\"\n\n\n\n    self.d_k=self.d_model // self.h\n\n    #  now query key and value weights\n\n    self.w_q=nn.Linear(d_model, d_model)\n\n    self.w_k=nn.Linear(d_model, d_model)\n\n    self.w_v=nn.Linear(d_model, d_model)\n\n    # matrix which we use after concatenating to convert it to same dimensional back\n\n    self.w_o=nn.Linear(d_model, d_model)\n\n    self.dropout=nn.Dropout(dropout)\n\n\n\n  @staticmethod\n\n  def attention(query, key, value, mask, dropout: nn.Dropout):\n\n    # mask => When we want certain words to NOT interact with others, we \"hide\" them\n\n\n\n    d_k = query.shape[-1] # The last dimension of query, key, and value\n\n\n\n    # We calculate the Attention(Q,K,V) as in the formula in the image above\n\n    attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n\n\n\n    # Before applying the softmax, we apply the mask to hide some interactions between words\n\n    if mask is not None: # If a mask IS defined...\n\n        attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n\n        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n\n        if dropout is not None: # If a dropout IS defined...\n\n            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n\n\n\n    return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n\n\n\n  def forward(self, q, k, v, mask):\n\n    query=self.w_q(q)\n\n    key=self.w_k(k)\n\n    value=self.w_v(v)\n\n\n\n\n\n    # reshaping it for multihead\n\n    query=query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n\n    key=key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n\n    value=value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n\n    # Obtaining the output and the attention scores\n\n    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n\n\n\n    # Obtaining the H matrix\n\n    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n\n\n    return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix","metadata":{"id":"iro6I3CdbWo7","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:11.229326Z","iopub.execute_input":"2024-11-12T07:33:11.229672Z","iopub.status.idle":"2024-11-12T07:33:11.243823Z","shell.execute_reply.started":"2024-11-12T07:33:11.229636Z","shell.execute_reply":"2024-11-12T07:33:11.242897Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**Residual Connections**","metadata":{"id":"9GhKsldVciqm"}},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n\n    def __init__(self, dropout: float) -> None:\n\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n\n        self.norm = LayerNormalization() # We use a normalization layer\n\n\n\n    def forward(self, x, sublayer):\n\n        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n\n        return x + self.dropout(self.norm(sublayer(x)))","metadata":{"id":"7HJuYxYvbWsB","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:12.556922Z","iopub.execute_input":"2024-11-12T07:33:12.557586Z","iopub.status.idle":"2024-11-12T07:33:12.563818Z","shell.execute_reply.started":"2024-11-12T07:33:12.557531Z","shell.execute_reply":"2024-11-12T07:33:12.562683Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**Feed Forward Network**","metadata":{"id":"EtPTepVzc2Ca"}},{"cell_type":"markdown","source":"dff=2048\n\nfrom the paper attention all you need","metadata":{"id":"5sQhXub4c8lj"}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n\n\n\n    def __init__(self, eps: float = 10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n\n        super().__init__()\n\n        self.eps = eps\n\n\n\n        # We define alpha as a trainable parameter and initialize it with ones\n\n        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n\n\n\n        # We define bias as a trainable parameter and initialize it with zeros\n\n        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n\n\n\n    def forward(self, x):\n\n        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n\n        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n\n\n\n        # Returning the normalized input\n\n        return self.alpha * (x-mean) / (std + self.eps) + self.bias","metadata":{"id":"a0hRXKjcxo1G","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:14.058151Z","iopub.execute_input":"2024-11-12T07:33:14.058507Z","iopub.status.idle":"2024-11-12T07:33:14.068715Z","shell.execute_reply.started":"2024-11-12T07:33:14.058474Z","shell.execute_reply":"2024-11-12T07:33:14.067751Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n\nclass FeedForwardBlock(nn.Module):\n\n\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n\n        super().__init__()\n\n        # First linear transformation\n\n        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n\n        self.dropout = nn.Dropout(dropout) # Dropout\n\n        # Second linear transformation\n\n        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n\n\n\n    def forward(self, x):\n\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))","metadata":{"id":"DmWJbtdfbWvK","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:14.484452Z","iopub.execute_input":"2024-11-12T07:33:14.484808Z","iopub.status.idle":"2024-11-12T07:33:14.491383Z","shell.execute_reply.started":"2024-11-12T07:33:14.484772Z","shell.execute_reply":"2024-11-12T07:33:14.490243Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Encoder Block**","metadata":{"id":"d0XbZiz0tyEo"}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n\n\n\n    def __init__(self, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n\n        super().__init__()\n\n        # Storing the self-attention block and feed-forward block\n\n        self.self_attention_block = self_attention_block\n\n        self.feed_forward_block = feed_forward_block\n\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n\n\n\n    def forward(self, x, src_mask):\n\n        # Applying the first residual connection with the self-attention block\n\n        # src_mask for padding tokens\n\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n\n        x = self.residual_connections[1](x, self.feed_forward_block)\n\n        return x","metadata":{"id":"q4rS16YJbWyA","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:15.511072Z","iopub.execute_input":"2024-11-12T07:33:15.512036Z","iopub.status.idle":"2024-11-12T07:33:15.518686Z","shell.execute_reply.started":"2024-11-12T07:33:15.511990Z","shell.execute_reply":"2024-11-12T07:33:15.517642Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**Complete Encoder**","metadata":{"id":"grN3hgOaEN9j"}},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n\n\n    # The Encoder takes in instances of 'EncoderBlock'\n\n    def __init__(self, layers: nn.ModuleList) -> None:\n\n        super().__init__()\n\n        self.layers = layers # Storing the EncoderBlocks\n\n        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n\n\n\n    def forward(self, x, mask):\n\n        # Iterating over each EncoderBlock stored in self.layers\n\n        for layer in self.layers:\n\n            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n\n        return self.norm(x) # Normalizing output","metadata":{"id":"Kvm3TNeRbW1L","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:16.558203Z","iopub.execute_input":"2024-11-12T07:33:16.558560Z","iopub.status.idle":"2024-11-12T07:33:16.565307Z","shell.execute_reply.started":"2024-11-12T07:33:16.558526Z","shell.execute_reply":"2024-11-12T07:33:16.564362Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n\nclass DecoderBlock(nn.Module):\n\n\n\n    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n\n    def __init__(self,  self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n\n        super().__init__()\n\n        self.self_attention_block = self_attention_block\n\n        self.cross_attention_block = cross_attention_block\n\n        self.feed_forward_block = feed_forward_block\n\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n\n\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n\n\n\n        # Self-Attention block with query, key, and value plus the target language mask\n\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n\n\n\n        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n\n\n\n        # Feed-forward block with residual connections\n\n        x = self.residual_connections[2](x, self.feed_forward_block)\n\n        return x","metadata":{"id":"GbLC7UuDbW4A","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:17.164226Z","iopub.execute_input":"2024-11-12T07:33:17.164587Z","iopub.status.idle":"2024-11-12T07:33:17.173490Z","shell.execute_reply.started":"2024-11-12T07:33:17.164551Z","shell.execute_reply":"2024-11-12T07:33:17.172527Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\n\n\n\nclass Decoder(nn.Module):\n\n\n\n    # The Decoder takes in instances of 'DecoderBlock'\n\n    def __init__(self, layers: nn.ModuleList) -> None:\n\n        super().__init__()\n\n        self.layers = layers\n\n        self.norm = LayerNormalization() # Layer to normalize the output\n\n\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n\n\n\n        # Iterating over each DecoderBlock stored in self.layers\n\n        for layer in self.layers:\n\n            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n\n        return self.norm(x) # Returns normalized output","metadata":{"id":"vuvGHK9_bW7I","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:17.718051Z","iopub.execute_input":"2024-11-12T07:33:17.718635Z","iopub.status.idle":"2024-11-12T07:33:17.724963Z","shell.execute_reply.started":"2024-11-12T07:33:17.718594Z","shell.execute_reply":"2024-11-12T07:33:17.724047Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**Linear Layer**","metadata":{"id":"mc8fDvXpOs4b"}},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n\n    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n\n        super().__init__()\n\n        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n\n    def forward(self, x):\n\n        return torch.log_softmax(self.proj(x), dim = -1)","metadata":{"id":"kt0uoBlobW9_","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:18.724218Z","iopub.execute_input":"2024-11-12T07:33:18.724945Z","iopub.status.idle":"2024-11-12T07:33:18.730536Z","shell.execute_reply.started":"2024-11-12T07:33:18.724901Z","shell.execute_reply":"2024-11-12T07:33:18.729525Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# **Transformer **","metadata":{"id":"MxN_WkkERCj8"}},{"cell_type":"code","source":"class Transformer(nn.Module):\n\n\n\n    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n\n    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: Embedding, tgt_embed: Embedding, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n\n        super().__init__()\n\n        self.encoder = encoder\n\n        self.decoder = decoder\n\n        self.src_embed = src_embed\n\n        self.tgt_embed = tgt_embed\n\n        self.src_pos = src_pos\n\n        self.tgt_pos = tgt_pos\n\n        self.projection_layer = projection_layer\n\n\n\n    # Encoder\n\n    def encode(self, src, src_mask):\n\n        src = self.src_embed(src) # Applying source embeddings to the input source language\n\n        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n\n        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n\n\n\n    # Decoder\n\n    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n\n        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n\n        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n\n\n\n        # Returning the target embeddings, the output of the encoder, and both source and target masks\n\n        # The target mask ensures that the model won't 'see' future elements of the sequence\n\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n\n\n\n    # Applying Projection Layer with the Softmax function to the Decoder output\n\n    def project(self, x):\n\n        return self.projection_layer(x)","metadata":{"id":"Gvk3CBbVbXA7","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:20.471511Z","iopub.execute_input":"2024-11-12T07:33:20.472299Z","iopub.status.idle":"2024-11-12T07:33:20.481518Z","shell.execute_reply.started":"2024-11-12T07:33:20.472253Z","shell.execute_reply":"2024-11-12T07:33:20.480336Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n\n\n\n    # Creating Embedding layers\n\n    src_embed = Embedding(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n\n    tgt_embed = Embedding(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n\n\n\n    # Creating Positional Encoding layers\n\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n\n\n\n    # Creating EncoderBlocks\n\n    encoder_blocks = [] # Initial list of empty EncoderBlocks\n\n    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n\n        encoder_self_attention_block = MultiHeadAttention(d_model, h, dropout) # Self-Attention\n\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n\n\n\n        # Combine layers into an EncoderBlock\n\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n\n        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n\n\n\n    # Creating DecoderBlocks\n\n    decoder_blocks = [] # Initial list of empty DecoderBlocks\n\n    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n\n        decoder_self_attention_block = MultiHeadAttention(d_model, h, dropout) # Self-Attention\n\n        decoder_cross_attention_block = MultiHeadAttention(d_model, h, dropout) # Cross-Attention\n\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n\n\n\n        # Combining layers into a DecoderBlock\n\n        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n\n        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n\n\n\n    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n\n    encoder = Encoder(nn.ModuleList(encoder_blocks))\n\n    decoder = Decoder(nn.ModuleList(decoder_blocks))\n\n\n\n    # Creating projection layer\n\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n\n\n\n    # Creating the transformer by combining everything above\n\n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n\n\n\n    # Initialize the parameters\n\n    for p in transformer.parameters():\n\n        if p.dim() > 1:\n\n            nn.init.xavier_uniform_(p)\n\n\n\n    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!\n","metadata":{"id":"rYWs_Xj3bXEK","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:21.394539Z","iopub.execute_input":"2024-11-12T07:33:21.395450Z","iopub.status.idle":"2024-11-12T07:33:21.407552Z","shell.execute_reply.started":"2024-11-12T07:33:21.395405Z","shell.execute_reply":"2024-11-12T07:33:21.406694Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def build_tokenizer(config, ds, lang):\n\n    # Crating a file path for the tokenizer\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n\n    # Checking if Tokenizer already exists\n    if not Path.exists(tokenizer_path):\n\n        # If it doesn't exist, we create a new one\n        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n\n        # Creating a trainer for the new tokenizer\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n\n        # Training new tokenizer on sentences from the dataset and language specified\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n    return tokenizer # Returns the loaded tokenizer or the trained tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:23.893006Z","iopub.execute_input":"2024-11-12T07:33:23.893388Z","iopub.status.idle":"2024-11-12T07:33:23.900312Z","shell.execute_reply.started":"2024-11-12T07:33:23.893352Z","shell.execute_reply":"2024-11-12T07:33:23.899424Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def casual_mask(size):\n        # Creating a square matrix of dimensions 'size x size' filled with ones\n        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n        return mask == 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:24.882895Z","iopub.execute_input":"2024-11-12T07:33:24.883735Z","iopub.status.idle":"2024-11-12T07:33:24.889121Z","shell.execute_reply.started":"2024-11-12T07:33:24.883685Z","shell.execute_reply":"2024-11-12T07:33:24.888134Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for pair in ds:\n        yield pair['translation'][lang]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:25.497648Z","iopub.execute_input":"2024-11-12T07:33:25.498579Z","iopub.status.idle":"2024-11-12T07:33:25.503336Z","shell.execute_reply.started":"2024-11-12T07:33:25.498526Z","shell.execute_reply":"2024-11-12T07:33:25.502025Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# def get_ds(config):\n\n#     # Loading the train portion of the OpusBooks dataset.\n#     # The Language pairs will be defined in the 'config' dictionary we will build later\n#     ds_raw = load_dataset('opus', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train')\n\n#     # Building or loading tokenizer for both the source and target languages\n#     tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n#     tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n#     # Splitting the dataset for training and validation\n#     train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n#     val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n#     train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n\n#     # Processing data with the BilingualDataset class, which we will define below\n#     train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n#     val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n#     # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n#     max_len_src = 0\n#     max_len_tgt = 0\n#     for pair in ds_raw:\n#         src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n#         tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n#         max_len_src = max(max_len_src, len(src_ids))\n#         max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n#     print(f'Max length of source sentence: {max_len_src}')\n#     print(f'Max length of target sentence: {max_len_tgt}')\n\n#     # Creating dataloaders for the training and validadion sets\n#     # Dataloaders are used to iterate over the dataset in batches during training and validation\n#     train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n#     val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n\n#     return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers\n\n     \n\n\n\n\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, random_split\n\ndef get_ds(config):\n    # Load the WMT2014 English-German dataset from Hugging Face Datasets\n    from datasets import load_from_disk\n\n    # Load the saved subset\n    ds_raw = load_from_disk(\"wmt14_en_de_subset_100k\")\n\n    # Verify the loaded subset\n    print(f\"Loaded subset size: {len(ds_raw)}\")\n\n    # ds_raw = load_dataset('wmt14','de-en', split='train')\n\n    # Initialize the tokenizers for source (English) and target (German) languages\n    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n    # Split the dataset for training and validation\n    train_ds_size = int(0.9 * len(ds_raw))  # 90% for training\n    val_ds_size = len(ds_raw) - train_ds_size  # 10% for validation\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    # Process data with the BilingualDataset class\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n    # Calculate the maximum sentence length in source and target sentences for reference\n    max_len_src = 0\n    max_len_tgt = 0\n    for pair in ds_raw:\n        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(pair['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n\n    # Create dataloaders for the training and validation sets\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:26.227409Z","iopub.execute_input":"2024-11-12T07:33:26.227869Z","iopub.status.idle":"2024-11-12T07:33:26.240316Z","shell.execute_reply.started":"2024-11-12T07:33:26.227804Z","shell.execute_reply":"2024-11-12T07:33:26.239254Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n\n    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n    # 'seq_len' defines the sequence length for both languages\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n\n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n\n        # Defining special tokens by using the target language tokenizer\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n\n    # Total number of instances in the dataset (some pairs are larger than others)\n    def __len__(self):\n        return len(self.ds)\n\n    # Using the index to retrive source and target texts\n    def __getitem__(self, index: Any) -> Any:\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n\n        # Tokenizing source and target texts\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        # Computing how many padding tokens need to be added to the tokenized texts\n        # Source tokens\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n        # Target tokens\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n\n        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n        # given the current sequence length limit (this will be defined in the config dictionary below)\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n\n        # Building the encoder input tensor by combining several elements\n        encoder_input = torch.cat(\n            [\n            self.sos_token, # inserting the '[SOS]' token\n            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n            self.eos_token, # Inserting the '[EOS]' token\n            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n        )\n\n        # Building the decoder input tensor by combining several elements\n        decoder_input = torch.cat(\n            [\n                self.sos_token, # inserting the '[SOS]' token\n                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n\n        )\n\n        # Creating a label tensor, the expected output for training the model\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n                self.eos_token, # Inserting the '[EOS]' token\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n\n            ]\n        )\n\n        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n            'label': label,\n            'src_text': src_text,\n            'tgt_text': tgt_text\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:27.143750Z","iopub.execute_input":"2024-11-12T07:33:27.144386Z","iopub.status.idle":"2024-11-12T07:33:27.160485Z","shell.execute_reply.started":"2024-11-12T07:33:27.144321Z","shell.execute_reply":"2024-11-12T07:33:27.159528Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    # Retrieving the indices from the start and end of sequences of the target tokens\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n\n    # Computing the output of the encoder for the source sequence\n    encoder_output = model.encode(source, source_mask)\n    # Initializing the decoder input with the Start of Sentence token\n    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n\n    # Looping until the 'max_len', maximum length, is reached\n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n\n        # Building a mask for the decoder input\n        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n        # Calculating the output of the decoder\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        # Applying the projection layer to get the probabilities for the next token\n        prob = model.project(out[:, -1])\n\n        # Selecting token with the highest probability\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n\n        # If the next token is an End of Sentence token, we finish the loop\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:28.045396Z","iopub.execute_input":"2024-11-12T07:33:28.046098Z","iopub.status.idle":"2024-11-12T07:33:28.056006Z","shell.execute_reply.started":"2024-11-12T07:33:28.046055Z","shell.execute_reply":"2024-11-12T07:33:28.054604Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n    model.eval() # Setting model to evaluation mode\n    count = 0 # Initializing counter to keep track of how many examples have been processed\n\n    console_width = 80 # Fixed witdh for printed messages\n\n    # Creating evaluation loop\n    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch['encoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n\n            # Ensuring that the batch_size of the validation set is 1\n            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n\n            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n            # Retrieving source and target texts from the batch\n            source_text = batch['src_text'][0]\n            target_text = batch['tgt_text'][0] # True translation\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n\n            # Printing results\n            print_msg('-'*console_width)\n            print_msg(f'SOURCE: {source_text}')\n            print_msg(f'TARGET: {target_text}')\n            print_msg(f'PREDICTED: {model_out_text}')\n\n            # After two examples, we break the loop\n            if count == num_examples:\n                break\n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:29.056919Z","iopub.execute_input":"2024-11-12T07:33:29.057717Z","iopub.status.idle":"2024-11-12T07:33:29.066449Z","shell.execute_reply.started":"2024-11-12T07:33:29.057677Z","shell.execute_reply":"2024-11-12T07:33:29.065508Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n\n    # Loading model using the 'build_transformer' function.\n    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model\n\n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T07:33:30.146515Z","iopub.execute_input":"2024-11-12T07:33:30.146926Z","iopub.status.idle":"2024-11-12T07:33:30.152054Z","shell.execute_reply.started":"2024-11-12T07:33:30.146885Z","shell.execute_reply":"2024-11-12T07:33:30.151099Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def get_config():\n    return{\n        'batch_size': 8,\n        'num_epochs': 10,\n        'lr': 10**-4,\n        'seq_len': 350,\n        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n        'lang_src': 'en',\n        'lang_tgt': 'de',\n        'model_folder': 'weights',\n        'model_basename': 'tmodel_00',\n        'preload': 4,\n        'tokenizer_file': 'tokenizer_{0}.json',\n        'experiment_name': 'runs/tmodel'\n    }\n\n\n# Function to construct the path for saving and retrieving model weights\ndef get_weights_file_path(config, epoch: str):\n    model_folder = config['model_folder'] # Extracting model folder from the config\n    model_basename = config['model_basename'] # Extracting the base name for model files\n    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename\n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T08:06:26.712090Z","iopub.execute_input":"2024-11-12T08:06:26.712759Z","iopub.status.idle":"2024-11-12T08:06:26.719981Z","shell.execute_reply.started":"2024-11-12T08:06:26.712717Z","shell.execute_reply":"2024-11-12T08:06:26.718897Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"def train_model(config):\n    # Setting up device to run on GPU to train faster\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device {device}\")\n\n    # Creating model directory to store weights\n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n\n    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n    # Initializing model on the GPU using the 'get_model' function\n    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n\n    # Tensorboard\n    writer = SummaryWriter(config['experiment_name'])\n\n    # Setting up the Adam optimizer with the specified learning rate from the '\n    # config' dictionary plus an epsilon value\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n\n    # Initializing epoch and global step variables\n    initial_epoch = 2\n    global_step = 0\n\n    # Checking if there is a pre-trained model to load\n    # If true, loads it\n    if config['preload']:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename) # Loading model\n        model.load_state_dict(state['model_state_dict'])\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n        print(f\"Resumed from epoch {initial_epoch}, global step {global_step}\")\n\n        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n        initial_epoch = state['epoch'] + 1\n        # # Loading the optimizer state from the saved model\n        # optimizer.load_state_dict(state['optimizer_state_dict'])\n        # print(optimizer.load_state_dict(state['optimizer_state_dict']))\n        # print(optimizer.state_dict())\n        # # Loading the global step state from the saved model\n        # global_step = state['global_step']\n\n    # Initializing CrossEntropyLoss function for training\n    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n    # We also apply label_smoothing to prevent overfitting\n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n\n    # Initializing training loop\n\n    # Iterating over each epoch from the 'initial_epoch' variable up to\n    # the number of epochs informed in the config\n    for epoch in range(initial_epoch, config['num_epochs']):\n\n        # Initializing an iterator over the training dataloader\n        # We also use tqdm to display a progress bar\n        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n\n        # For each batch...\n        for batch in batch_iterator:\n            model.train() # Train the model\n\n            # Loading input data and masks onto the GPU\n            encoder_input = batch['encoder_input'].to(device)\n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n\n            # Running tensors through the Transformer\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            # Loading the target labels onto the GPU\n            label = batch['label'].to(device)\n\n            # Computing loss between model's output and true labels\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n\n            # Updating progress bar\n            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n\n            writer.add_scalar('train loss', loss.item(), global_step)\n            writer.flush()\n\n            # Performing backpropagation\n            loss.backward()\n\n            # Updating parameters based on the gradients\n            optimizer.step()\n\n            # Clearing the gradients to prepare for the next batch\n            optimizer.zero_grad()\n\n            global_step += 1 # Updating global step count\n\n        # We run the 'run_validation' function at the end of each epoch\n        # to evaluate model performance\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n\n        # Saving model\n        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n        # Writting current model state to the 'model_filename'\n        torch.save({\n            'epoch': epoch, # Current epoch\n            'model_state_dict': model.state_dict(),# Current model state\n            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n            'global_step': global_step # Current global step\n        }, model_filename)\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T08:06:27.268427Z","iopub.execute_input":"2024-11-12T08:06:27.269147Z","iopub.status.idle":"2024-11-12T08:06:27.287916Z","shell.execute_reply.started":"2024-11-12T08:06:27.269103Z","shell.execute_reply":"2024-11-12T08:06:27.286752Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def greedy(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    # Retrieve the start and end of sequence tokens\n    sos_idx = tokenizer_tgt.convert_tokens_to_ids('[SOS]')  # Start of Sentence\n    eos_idx = tokenizer_tgt.convert_tokens_to_ids('[EOS]')  # End of Sentence\n\n    # Initialize decoder input with the SOS token\n    decoder_input = torch.tensor([[sos_idx]], device=device)\n\n    # Prepare to store the generated tokens\n    generated_tokens = []\n\n    with torch.no_grad():\n        # Pass the source through the encoder\n        encoder_output = model.encode(source, source_mask)\n\n        for _ in range(max_len):\n            # Create a decoder mask\n            decoder_mask = (decoder_input != tokenizer_src.pad_token_id).unsqueeze(1).to(device)\n\n            # Pass encoder output and decoder input through the model to get output\n            decoder_output = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            # Get the next token (greedy selection)\n            next_token = proj_output.argmax(-1)[:, -1].item()\n            generated_tokens.append(next_token)\n\n            # Stop if EOS token is generated\n            if next_token == eos_idx:\n                break\n\n            # Add the predicted token to decoder input for the next iteration\n            decoder_input = torch.cat([decoder_input, torch.tensor([[next_token]], device=device)], dim=1)\n\n    return torch.tensor(generated_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T08:06:28.798776Z","iopub.execute_input":"2024-11-12T08:06:28.799179Z","iopub.status.idle":"2024-11-12T08:06:28.808313Z","shell.execute_reply.started":"2024-11-12T08:06:28.799141Z","shell.execute_reply":"2024-11-12T08:06:28.807307Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"config=get_config()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# 2. Load tokenizers and data\ntrain_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n# 3. Initialize the model and load weights if available\nmodel = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n\n# Check if there are pre-trained weights to load\nif config.get('preload'):\n    model_filename = get_weights_file_path(config, config['preload'])\n    print(f'Loading pre-trained model from {model_filename}')\n    state = torch.load(model_filename)\n    model.load_state_dict(state['model_state_dict'])\n\n# 4. Set the maximum sequence length for translation\nmax_len = config['seq_len']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T14:06:19.764309Z","iopub.execute_input":"2024-11-12T14:06:19.764697Z","iopub.status.idle":"2024-11-12T14:06:35.644066Z","shell.execute_reply.started":"2024-11-12T14:06:19.764657Z","shell.execute_reply":"2024-11-12T14:06:35.642990Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded subset size: 100000\nMax length of source sentence: 300\nMax length of target sentence: 266\nLoading pre-trained model from weights/tmodel_004.pt\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\nimport torch\n\n# Load tokenizers\ntokenizer_src = PreTrainedTokenizerFast(tokenizer_file=\"/kaggle/working/tokenizer_en.json\")\ntokenizer_tgt = PreTrainedTokenizerFast(tokenizer_file=\"/kaggle/working/tokenizer_de.json\")\n\n# Set special tokens if they're not automatically set\nif tokenizer_src.pad_token is None:\n    tokenizer_src.pad_token = \"[PAD]\"\nif tokenizer_tgt.pad_token is None:\n    tokenizer_tgt.pad_token = \"[PAD]\"\n\n# Define the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load model architecture and weights\ndef load_model(config):\n    model = get_model(config, tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n    model_path = \"/kaggle/working/weights/tmodel_0008.pt\"\n    checkpoint = torch.load(model_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    return model\n\n# Translation inference function\ndef translate_sentence_inference(model, sentence, tokenizer_src, tokenizer_tgt, max_len, device):\n    model.eval()  # Set model to evaluation mode\n\n    # Tokenize the input sentence and convert it to a tensor\n    input_tokens = tokenizer_src.encode(sentence, add_special_tokens=True)  # Returns list of token IDs\n    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Add batch dimension\n\n    # Set pad_token_id\n    pad_token_id = tokenizer_src.pad_token_id if tokenizer_src.pad_token_id is not None else 0\n    encoder_mask = (input_tensor != pad_token_id).unsqueeze(1).to(device)  # Create mask\n\n    # Generate translation using the greedy decoding method\n    model_out = greedy(model, input_tensor, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n    # Decode the output tokens to get the translated sentence\n    translated_sentence = tokenizer_tgt.decode(model_out.cpu().numpy(), skip_special_tokens=True)\n\n    # Print the results\n    print('-' * 80)\n    print(f'SOURCE: {sentence}')\n    print(f'TRANSLATION: {translated_sentence}')\n    print('-' * 80)\n\n    return translated_sentence\n\n\n\nmodel = load_model(config)\nenglish_sentence = \"English is crucial for everyone who wants to prosper in the increasingly globalized world.\"\ntranslated_sentence = translate_sentence_inference(model, english_sentence, tokenizer_src, tokenizer_tgt, max_len=50, device=device)\nprint(\"Translated sentence:\", translated_sentence)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T14:11:45.406136Z","iopub.execute_input":"2024-11-12T14:11:45.406602Z","iopub.status.idle":"2024-11-12T14:11:47.928307Z","shell.execute_reply.started":"2024-11-12T14:11:45.406555Z","shell.execute_reply":"2024-11-12T14:11:47.927331Z"}},"outputs":[{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: English is crucial for everyone who wants to prosper in the increasingly globalized world.\nTRANSLATION: Jeder entscheidende Grund fr alle , die sich in der Welt entwickeln will .\n--------------------------------------------------------------------------------\nTranslated sentence: Jeder entscheidende Grund fr alle , die sich in der Welt entwickeln will .\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings('ignore') # Filtering warnings\nconfig = get_config() # Retrieving config settings\ntrain_model(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T08:06:33.799371Z","iopub.execute_input":"2024-11-12T08:06:33.799769Z","iopub.status.idle":"2024-11-12T14:05:12.691818Z","shell.execute_reply.started":"2024-11-12T08:06:33.799728Z","shell.execute_reply":"2024-11-12T14:05:12.690344Z"}},"outputs":[{"name":"stdout","text":"Using device cuda\nLoaded subset size: 100000\nMax length of source sentence: 300\nMax length of target sentence: 266\nPreloading model weights/tmodel_004.pt\nResumed from epoch 2, global step 56250\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 05: 100%|| 11250/11250 [1:29:34<00:00,  2.09it/s, loss=3.878]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: This type of project complies with the principle of regional cohesion and solidarity, which we have always defended, and, as such, should enjoy the widespread support of the European Union in the context of the promotion of trans-European networks and regional policy (through the Structural Funds).\nTARGET: Derartige Projekte entsprechen dem Grundsatz des Zusammenhalts und der Solidaritt zwischen den Regionen, den wir stets verteidigt haben. In diesem Sinne sollte solche Projekte weitgehend von der Europischen Union untersttzt werden, im Rahmen der Frderung der transeuropischen Netze und der Regionalpolitik (mittels der Strukturfonds).\nPREDICTED: Diese Art von Zielen des regionalen Zusammenhalts und der Solidaritt , die wir stets verteidigt haben , mssen wir stets die allgemeinen Untersttzung der Europischen Union im Rahmen der Strukturfonds und der Regionalpolitik der Europischen Union ( KOM ( 1999 ) B - und Regionalpolitik ) ( KOM ( 1999 ) B ).\n--------------------------------------------------------------------------------\nSOURCE: These clauses can be used constructively, but they need to be more closely defined with respect to application, implementation and sanctions.\nTARGET: Diese Klauseln knnen konstruktiv genutzt werden, mssen jedoch in bezug auf Anwendung, Umsetzung und Sanktionen przisiert werden.\nPREDICTED: Diese Programme knnen konstruktiv genutzt werden , aber sie mssen sich mit der Einhaltung der Umsetzung und Sanktionen befassen .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 06: 100%|| 11250/11250 [1:29:32<00:00,  2.09it/s, loss=4.111]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: A quarter of the country' s four million people are estimated to have travelled abroad in search of work.\nTARGET: Man kann davon ausgehen, da ein Viertel der vier Millionen Moldauer sich auf Beschftigungssuche ins Ausland begeben hat.\nPREDICTED: Ein Viertel der vier Millionen Menschen sind im Ausland im Ausland im Ausland .\n--------------------------------------------------------------------------------\nSOURCE: There are also other matters which I would like to stress, which do not appear in the proposal on prices, such as my concern for the nut sector, hazelnuts and almonds, which is facing such great problems at the moment, since, in some cases, the improvement plans implemented by the Commission are coming to an end.\nTARGET: Es gibt auch noch weitere Fragen, auf die ich aufmerksam machen mchte und die nicht im Preisvorschlag enthalten sind, so meine Sorge um den Sektor Schalenobst, Haselnsse und Mandeln, die derzeit so groe Probleme verursachen, weil in einigen Fllen die von der Kommission gewhrten Verbesserungsplne auslaufen.\nPREDICTED: Auerdem sind andere Dinge , die ich hervorheben mchte , die im Vorschlag fr die Preise , wie ich meine Besorgnis ber die , die und die , die gegenwrtig sehr groe Probleme bereiten , die in einigen Fllen , die die von der Kommission umgesetzt werden , zu einer besseren Umsetzung der Plne der Kommission zur Verfgung stehen .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 07: 100%|| 11250/11250 [1:29:27<00:00,  2.10it/s, loss=3.082]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: The reason given, and also outlined by my colleague Mr Elles, is that we believe that this will pre-empt discussions with accession states.\nTARGET: Das wird, wie u. a. im Beitrag meines Kollegen Herrn Elles deutlich wurde, damit begrndet, da dies unserer Ansicht nach die Diskussionen mit den Beitrittslndern beeinflussen knnte.\nPREDICTED: Der Grund dafr , und auch mein Kollege Elles , ist der Ansicht , da wir die Diskussionen mit den Beitrittsverhandlungen mit den Beitrittsverhandlungen errtern werden .\n--------------------------------------------------------------------------------\nSOURCE: It is really tragic what is happening.\nTARGET: Es ist wirklich tragisch, was hier passiert.\nPREDICTED: Es ist wirklich tragisch , was passiert .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 08: 100%|| 11250/11250 [1:29:29<00:00,  2.10it/s, loss=3.290]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: Of course we need to modernise these systems.\nTARGET: Natrlich mssen wir diese Systeme anpassen.\nPREDICTED: Natrlich mssen wir diese Systeme modernisieren .\n--------------------------------------------------------------------------------\nSOURCE: At his invitation I myself took part in a meeting between the national parliaments and the European Parliament, where I found a great deal of interest expressed. The Commission will be prepared to take part in any exercise aimed at enhancing this dialogue and this mutual understanding between the national parliaments, which we need in order to ratify a genuine reform, and the European Parliament.\nTARGET: Ich selbst habe aufgrund seiner Einladung an einer Zusammenkunft der nationalen Parlamente und des Europischen Parlaments teilgenommen, die ich sehr interessant fand, und die Kommission ist bereit zur Teilnahme an jeglichen Treffen, die zur Vertiefung dieses Dialogs und dieser gegenseitigen Verstndigung zwischen den nationalen Parlamenten, deren Untersttzung wir zur Ratifizierung einer echten Reform bentigen, und dem Europischen Parlament beitragen.\nPREDICTED: Ich selbst habe selbst an einer Sitzung teilgenommen , in der ein Treffen zwischen den nationalen Parlamenten teilgenommen hat , wo ich sehr viel Interesse an dieser Tagung teilnehmen werde , und die Kommission wird sich auf jeden Fall auf einen Dialog zwischen den nationalen Parlamenten vorbereitet , die eine wirkliche Reform und die Ratifizierung einer echten Reform des Europischen Parlaments verdient .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 09:   0%|          | 17/11250 [00:08<1:33:08,  2.01it/s, loss=3.236]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[68], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Filtering warnings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config() \u001b[38;5;66;03m# Retrieving config settings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[64], line 89\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     86\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Performing backpropagation\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Updating parameters based on the gradients\u001b[39;00m\n\u001b[1;32m     92\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":68},{"cell_type":"code","source":"# import torch\n\n# def translate_sentence(sentence, model, tokenizer_src, tokenizer_tgt, device, max_length=50):\n#     model.eval()  # Set the model to evaluation mode\n\n#     # Tokenize the input sentence and convert to tensor\n#     input_tokens = tokenizer_src.encode(sentence)\n#     input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Add batch dimension\n\n#     # Create an encoder mask (assuming padding token ID is 0)\n#     encoder_mask = (input_tensor != tokenizer_src.token_to_id('[PAD]')).unsqueeze(1).to(device)\n\n#     # Pass through the encoder\n#     with torch.no_grad():\n#         encoder_output = model.encode(input_tensor, encoder_mask)\n\n#     # Initialize the decoder input with the start token\n#     decoder_input = torch.tensor([tokenizer_tgt.token_to_id('[START]')]).unsqueeze(0).to(device)\n\n#     # Prepare to store generated tokens\n#     generated_tokens = []\n\n#     for _ in range(max_length):\n#         # Create a decoder mask\n#         decoder_mask = (decoder_input != tokenizer_tgt.token_to_id('[PAD]')).unsqueeze(1).to(device)\n\n#         # Decode the encoder output\n#         with torch.no_grad():\n#             decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n#             proj_output = model.project(decoder_output)\n\n#         # Get the token with the highest probability\n#         next_token = proj_output.argmax(-1)[:, -1].item()\n#         generated_tokens.append(next_token)\n\n#         # Break if the end token is generated\n#         if next_token == tokenizer_tgt.token_to_id('[END]'):\n#             break\n\n#         # Append the predicted token to the decoder input for the next iteration\n#         decoder_input = torch.cat([decoder_input, torch.tensor([[next_token]]).to(device)], dim=1)\n\n#     # Decode the generated tokens to words\n#     output_sentence = tokenizer_tgt.decode(generated_tokens)\n\n#     return output_sentence\n\n# # Usage example\n# # Assuming 'model', 'tokenizer_src', and 'tokenizer_tgt' are already loaded, and 'device' is set\n# english_sentence = \"Hello, how are you?\"\n# translated_sentence = translate_sentence(english_sentence, model, tokenizer_src, tokenizer_tgt, device)\n# print(\"Translated sentence:\", translated_sentence)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the WMT2014 dataset for English-German translation\ndataset = load_dataset(\"wmt14\", \"de-en\", split=\"train\")\n\n# Select 100,000 samples from the dataset\nsubset_size = 100000\nsubset = dataset.select(range(subset_size))  # This will take the first 100,000 samples\n\n# Check the subset size\nprint(f\"Subset size: {len(subset)}\")\n\n# Optional: Save the subset to disk for later use\nsubset.save_to_disk(\"wmt14_en_de_subset_100k\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef build_tokenizer(config, lang):\n    if lang == config['lang_src']:\n        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    elif lang == config['lang_tgt']:\n        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n    else:\n        raise ValueError(\"Unsupported language\")\n\n    # Adding special tokens\n    special_tokens = {'additional_special_tokens': ['[SOS]', '[EOS]'], 'pad_token': '[PAD]'}\n    tokenizer.add_special_tokens(special_tokens)\n\n    print(f\"Tokenizer vocab size after adding special tokens: {tokenizer.vocab_size}\")\n    return tokenizer\n\n\ndef casual_mask(size):\n    \"\"\"Creates a future-masking matrix to prevent decoder from 'seeing' future tokens.\"\"\"\n    return torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool)\n\n\ndef train_model(config, df):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, df)\n\n    model = get_model(config, tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n    model.src_embed.embedding = nn.Embedding(tokenizer_src.vocab_size, config['d_model'])\n    model.tgt_embed.embedding = nn.Embedding(tokenizer_tgt.vocab_size, config['d_model'])\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.pad_token_id, label_smoothing=0.1).to(device)\n\n    for epoch in range(config['num_epochs']):\n        model.train()\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n            optimizer.zero_grad()\n            encoder_input, decoder_input = batch['encoder_input'].to(device), batch['decoder_input'].to(device)\n            encoder_mask, decoder_mask = batch['encoder_mask'].to(device), batch['decoder_mask'].to(device)\n            label = batch['label'].to(device)\n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.vocab_size), label.view(-1))\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n            optimizer.step()\n","metadata":{"id":"qhPrx_JzsFD4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndef build_tokenizer(config, lang):\n    if lang == config['lang_src']:\n        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # English tokenizer\n    elif lang == config['lang_tgt']:\n        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")  # German tokenizer\n    else:\n        raise ValueError(\"Unsupported language\")\n    return tokenizer\n\n\n","metadata":{"id":"Oruq6Y7JEzWl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def casual_mask(size):\n\n#         # Creating a square matrix of dimensions 'size x size' filled with ones\n\n#         mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n\n#         return mask == 0\n\ndef casual_mask(size):\n    \"\"\"Creates a causal mask of shape (1, size, size) to prevent attention to future tokens.\"\"\"\n    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.bool)\n    return mask == 0  # Invert the mask so that future positions are masked (set to False)\n","metadata":{"id":"6DJvrTH7FFEn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nclass BilingualDataset(Dataset):\n    def __init__(self, df, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        self.df = df\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.seq_len = seq_len\n        self.sos_token_id = tokenizer_tgt.convert_tokens_to_ids(\"[SOS]\")\n        self.eos_token_id = tokenizer_tgt.convert_tokens_to_ids(\"[EOS]\")\n        self.pad_token_id = tokenizer_tgt.pad_token_id\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        src_text = self.df.iloc[index][self.src_lang]\n        tgt_text = self.df.iloc[index][self.tgt_lang]\n\n        # Tokenize source and target texts\n        enc_input_tokens = self.tokenizer_src.encode(src_text, truncation=True, padding=\"max_length\", max_length=self.seq_len)\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text, truncation=True, padding=\"max_length\", max_length=self.seq_len)\n\n        # Convert to tensors\n        encoder_input = torch.tensor(enc_input_tokens, dtype=torch.long)\n        decoder_input = torch.tensor([self.sos_token_id] + dec_input_tokens[:-1], dtype=torch.long)  # Add [SOS] at the beginning\n        label = torch.tensor(dec_input_tokens, dtype=torch.long)\n\n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'encoder_mask': (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n            'decoder_mask': (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n            'label': label,\n            'src_text': src_text,\n            'tgt_text': tgt_text\n        }\n\n\n\n\n# class BilingualDataset(Dataset):\n#     def __init__(self, df, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n#         self.df = df\n#         self.tokenizer_src = tokenizer_src\n#         self.tokenizer_tgt = tokenizer_tgt\n#         self.src_lang = src_lang\n#         self.tgt_lang = tgt_lang\n#         self.seq_len = seq_len\n#         self.sos_token_id = self.tokenizer_tgt.convert_tokens_to_ids(\"[SOS]\")\n#         self.eos_token_id = self.tokenizer_tgt.convert_tokens_to_ids(\"[EOS]\")\n#         self.pad_token_id = self.tokenizer_tgt.pad_token_id\n\n#     def __getitem__(self, index):\n#         src_text = self.df.iloc[index][self.src_lang]\n#         tgt_text = self.df.iloc[index][self.tgt_lang]\n\n#         # Tokenize source and target texts\n#         enc_input_tokens = self.tokenizer_src.encode(src_text, truncation=True, padding=\"max_length\", max_length=self.seq_len)\n#         dec_input_tokens = self.tokenizer_tgt.encode(tgt_text, truncation=True, padding=\"max_length\", max_length=self.seq_len)\n\n#         # Convert to tensors\n#         encoder_input = torch.tensor(enc_input_tokens, dtype=torch.long)\n#         decoder_input = torch.tensor([self.sos_token_id] + dec_input_tokens[:-1], dtype=torch.long)  # Add [SOS] at the beginning\n#         label = torch.tensor(dec_input_tokens + [self.eos_token_id], dtype=torch.long)  # Add [EOS] at the end\n\n#         return {\n#             'encoder_input': encoder_input,\n#             'decoder_input': decoder_input,\n#             'encoder_mask': (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n#             'decoder_mask': (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n#             'label': label,\n#             'src_text': src_text,\n#             'tgt_text': tgt_text\n#         }\n","metadata":{"id":"DPrhyf-1Ezct","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef get_dataloaders(config, df):\n    tokenizer_src = build_tokenizer(config, config['lang_src'])\n    tokenizer_tgt = build_tokenizer(config, config['lang_tgt'])\n    train_size = int(0.9 * len(df))\n    train_df = df[:train_size]\n    val_df = df[train_size:]\n\n    train_ds = BilingualDataset(train_df, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_df, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=1, shuffle=True)\n\n    return train_loader, val_loader, tokenizer_src, tokenizer_tgt\n","metadata":{"id":"WNZ0IuyvEzfD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    model.resize_token_embeddings(len(tokenizer_src))\n    return model\n","metadata":{"id":"H7DWG-4XEzho","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"EPi2acy2Ezj3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef get_config():\n\n    return{\n\n        'batch_size': 6,\n\n        'num_epochs': 20,\n\n        'lr': 10**-4,\n\n        'seq_len': 350,\n\n        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n\n        'lang_src': 'en',\n\n        'lang_tgt': 'de',\n\n        'model_folder': '/kaggle/working/weights',\n\n        'model_basename': '/kaggle/working/tmodel_',\n\n        'preload': None,\n\n        'tokenizer_file': 'tokenizer_{0}.json',\n\n        'experiment_name': 'runs/tmodel'\n\n    }\n\n\n\n\n\n# Function to construct the path for saving and retrieving model weights\n\ndef get_weights_file_path(config, epoch: str):\n\n    model_folder = config['model_folder'] # Extracting model folder from the config\n\n    model_basename = config['model_basename'] # Extracting the base name for model files\n\n    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n\n    return str(Path('.')/ model_folder/ model_filename)\n\n\ndef greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.convert_tokens_to_ids('[SOS]')\n    eos_idx = tokenizer_tgt.convert_tokens_to_ids('[EOS]')\n\n    encoder_output = model.encode(source, source_mask)\n    decoder_input = torch.full((1, 1), sos_idx, dtype=torch.long).to(device)\n\n    while True:\n        if decoder_input.size(1) >= max_len:\n            break\n\n        decoder_mask = casual_mask(decoder_input.size(1)).to(device)\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n        prob = model.project(out[:, -1])\n\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat([decoder_input, next_word.unsqueeze(0)], dim=1)\n\n        if next_word.item() == eos_idx:\n            break\n\n    return decoder_input.squeeze(0)\n\n\ndef run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n\n    model.eval() # Setting model to evaluation mode\n\n    count = 0 # Initializing counter to keep track of how many examples have been processed\n\n\n\n    console_width = 80 # Fixed witdh for printed messages\n\n\n\n    # Creating evaluation loop\n\n    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n\n        for batch in validation_ds:\n\n            count += 1\n\n            encoder_input = batch['encoder_input'].to(device)\n\n            encoder_mask = batch['encoder_mask'].to(device)\n\n\n\n            # Ensuring that the batch_size of the validation set is 1\n\n            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n\n\n\n            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n\n\n            # Retrieving source and target texts from the batch\n\n            source_text = batch['src_text'][0]\n\n            target_text = batch['tgt_text'][0] # True translation\n\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n\n\n\n            # Printing results\n\n            print_msg('-'*console_width)\n\n            print_msg(f'SOURCE: {source_text}')\n\n            print_msg(f'TARGET: {target_text}')\n\n            print_msg(f'PREDICTED: {model_out_text}')\n\n\n\n            # After two examples, we break the loop\n\n            if count == num_examples:\n\n                break\n\nimport pandas as pd\n\n\n\nfile_path = '/kaggle/input/less-train/less-train.csv'\n\ndf = pd.read_csv(file_path)\n\ndf=df.iloc[:100000,:]\n\n# Drop rows with NaN in 'en' or 'de' columns\n\ndf = df.dropna(subset=['en', 'de'])\n","metadata":{"id":"Q0BDUcQtsFKP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n\n\n\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n\n    return model","metadata":{"id":"xLGLQ5r9sFN6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef get_ds(config, df):\n    # Get dataloaders and tokenizers for source and target languages\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataloaders(config, df)\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n\n# Training function that now accepts `df` for training data\ndef train_model(config, df):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device {device}\")\n\n    # Creating model directory to store weights\n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n\n    # Retrieving dataloaders and tokenizers for source and target languages\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, df)\n\n    # Initializing model on the GPU\n    model = get_model(config, tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n\n    # Tensorboard setup\n    writer = SummaryWriter(config['experiment_name'])\n\n    # Setting up the optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n\n    # Initializing epoch and global step variables\n    initial_epoch = 0\n    global_step = 0\n\n    # Checking if there is a pre-trained model to load\n    if config['preload']:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename)\n\n        # Restore saved states\n        initial_epoch = state['epoch'] + 1\n        model.load_state_dict(state['model_state_dict'])\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n\n    # Setting up CrossEntropyLoss with label smoothing and padding token ignore\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.convert_tokens_to_ids('[PAD]'), label_smoothing=0.1).to(device)\n\n    # Training loop\n    for epoch in range(initial_epoch, config['num_epochs']):\n        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n        for batch in batch_iterator:\n            model.train()\n\n            # Move data to GPU\n            encoder_input = batch['encoder_input'].to(device)\n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n            label = batch['label'].to(device)\n\n            # Forward pass\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            # Calculate loss\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.vocab_size), label.view(-1))\n            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n\n            writer.add_scalar('train loss', loss.item(), global_step)\n            writer.flush()\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n\n        # Run validation at the end of each epoch\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device,\n                       lambda msg: batch_iterator.write(msg), global_step, writer)\n\n        # Save model checkpoint\n        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)\n        print(f\"Checkpoint saved at {model_filename}\")\n","metadata":{"id":"Wi2V_eh6sFRQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings('ignore') # Filtering warnings\nconfig = get_config() # Retrieving config settings\n# train_model(config, df)","metadata":{"id":"9pVM43RjsFUa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"pUJRk073sFX3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# writer = SummaryWriter(config['experiment_name'])\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# # model_filename = get_weights_file_path(config, config['preload'])\n# state = torch.load(\"tmodel_00.pt\")\n# global_step = state['global_step']\n# train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, df)\n# model = get_model(config, tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n# print()\n# run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device,\n#                        lambda msg: batch_iterator.write(msg), global_step, writer)","metadata":{"id":"X9N1lX3nsFbO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"U8DwQqZssFeX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"D7NTkoP_sFhl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"2Eg2hXc0sFk_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"KKm_MCaasFoU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"rgkqFSjPsFsd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"_KJcwuzksFwj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Tokenizer function modified for DataFrame usage\n\n# def build_tokenizer(config, df, lang):\n\n#     tokenizer_path = Path(config['tokenizer_file'].format(lang))\n\n\n\n#     if not Path.exists(tokenizer_path):\n\n#         tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n\n#         tokenizer.pre_tokenizer = Whitespace()\n\n\n\n#         # Training the tokenizer on sentences from the DataFrame\n\n#         trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n\n#         tokenizer.train_from_iterator(df[lang].tolist(), trainer=trainer)\n\n#         tokenizer.save(str(tokenizer_path))\n\n#     else:\n\n#         tokenizer = Tokenizer.from_file(str(tokenizer_path))\n\n#     return tokenizer\n\n\n\n\n\n# def get_ds(config, df):\n\n#     # Using the DataFrame `df` instead of loading from Hugging Face\n\n#     tokenizer_src = build_tokenizer(config, df, config['lang_src'])\n\n#     tokenizer_tgt = build_tokenizer(config, df, config['lang_tgt'])\n\n\n\n#     # Splitting the dataset for training and validation\n\n#     train_ds_size = int(0.9 * len(df))\n\n#     train_df = df[:train_ds_size]\n\n#     val_df = df[train_ds_size:]\n\n\n\n#     # Creating dataset instances for training and validation\n\n#     train_ds = BilingualDataset(train_df, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n#     val_ds = BilingualDataset(val_df, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n\n\n#     # Dataloaders\n\n#     train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n\n#     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n\n\n\n#     return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n\n\n\n\n\n# class BilingualDataset(Dataset):\n\n#     def __init__(self, df, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n\n#         super().__init__()\n\n#         self.seq_len = seq_len\n\n#         self.df = df\n\n#         self.tokenizer_src = tokenizer_src\n\n#         self.tokenizer_tgt = tokenizer_tgt\n\n#         self.src_lang = src_lang\n\n#         self.tgt_lang = tgt_lang\n\n#         self.sos_token = torch.tensor([tokenizer_tgt.convert_tokens_to_ids(\"[SOS]\")], dtype=torch.int64)\n\n#         self.eos_token = torch.tensor([tokenizer_tgt.convert_tokens_to_ids(\"[EOS]\")], dtype=torch.int64)\n\n#         self.pad_token = torch.tensor([tokenizer_tgt.convert_tokens_to_ids(\"[PAD]\")], dtype=torch.int64)\n\n\n\n#     def __len__(self):\n\n#         return len(self.df)\n\n\n\n#     def __getitem__(self, index: Any) -> Any:\n\n#         src_text = self.df.iloc[index][self.src_lang]\n\n#         tgt_text = self.df.iloc[index][self.tgt_lang]\n\n\n\n#         # Tokenizing\n\n#         enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n\n#         dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n\n\n#         enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n\n#         dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n\n\n\n#         if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n\n#             raise ValueError('Sentence is too long')\n\n\n\n#         encoder_input = torch.cat(\n\n#             [self.sos_token, torch.tensor(enc_input_tokens, dtype=torch.int64), self.eos_token,\n\n#              torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)]\n\n#         )\n\n#         decoder_input = torch.cat(\n\n#             [self.sos_token, torch.tensor(dec_input_tokens, dtype=torch.int64),\n\n#              torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)]\n\n#         )\n\n#         label = torch.cat(\n\n#             [torch.tensor(dec_input_tokens, dtype=torch.int64), self.eos_token,\n\n#              torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)]\n\n#         )\n\n\n\n#         assert encoder_input.size(0) == self.seq_len\n\n#         assert decoder_input.size(0) == self.seq_len\n\n#         assert label.size(0) == self.seq_len\n\n\n\n#         return {\n\n#             'encoder_input': encoder_input,\n\n#             'decoder_input': decoder_input,\n\n#             'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n\n#             'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n\n#             'label': label,\n\n#             'src_text': src_text,\n\n#             'tgt_text': tgt_text\n\n#         }\n\n\n\n# # Now pass `df` as an argument to `get_ds` in the `train_model` function:\n\n# train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, df)\n\n\n\n# # Iterating through dataset to extract the original sentence and its translation\n\n# def get_all_sentences(ds, lang):\n\n#     for pair in ds:\n\n#         yield pair['translation'][lang]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# def casual_mask(size):\n\n#         # Creating a square matrix of dimensions 'size x size' filled with ones\n\n#         mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n\n#         return mask == 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n\n#     # Retrieving the indices from the start and end of sequences of the target tokens\n\n#     sos_idx = tokenizer_tgt.convert_tokens_to_ids('[SOS]')\n\n#     eos_idx = tokenizer_tgt.convert_tokens_to_ids('[EOS]')\n\n\n\n#     # Computing the output of the encoder for the source sequence\n\n#     encoder_output = model.encode(source, source_mask)\n\n#     # Initializing the decoder input with the Start of Sentence token\n\n#     decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n\n\n\n#     # Looping until the 'max_len', maximum length, is reached\n\n#     while True:\n\n#         if decoder_input.size(1) == max_len:\n\n#             break\n\n\n\n#         # Building a mask for the decoder input\n\n#         decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n\n\n#         # Calculating the output of the decoder\n\n#         out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n\n\n#         # Applying the projection layer to get the probabilities for the next token\n\n#         prob = model.project(out[:, -1])\n\n\n\n#         # Selecting token with the highest probability\n\n#         _, next_word = torch.max(prob, dim=1)\n\n#         decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n\n\n\n#         # If the next token is an End of Sentence token, we finish the loop\n\n#         if next_word == eos_idx:\n\n#             break\n\n\n\n#     return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder\n\n\n\n\n\n\n\n\n\n\n\n\n\n# def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n\n#     model.eval() # Setting model to evaluation mode\n\n#     count = 0 # Initializing counter to keep track of how many examples have been processed\n\n\n\n#     console_width = 80 # Fixed witdh for printed messages\n\n\n\n#     # Creating evaluation loop\n\n#     with torch.no_grad(): # Ensuring that no gradients are computed during this process\n\n#         for batch in validation_ds:\n\n#             count += 1\n\n#             encoder_input = batch['encoder_input'].to(device)\n\n#             encoder_mask = batch['encoder_mask'].to(device)\n\n\n\n#             # Ensuring that the batch_size of the validation set is 1\n\n#             assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n\n\n\n#             # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n\n#             model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n\n\n#             # Retrieving source and target texts from the batch\n\n#             source_text = batch['src_text'][0]\n\n#             target_text = batch['tgt_text'][0] # True translation\n\n#             model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n\n\n\n#             # Printing results\n\n#             print_msg('-'*console_width)\n\n#             print_msg(f'SOURCE: {source_text}')\n\n#             print_msg(f'TARGET: {target_text}')\n\n#             print_msg(f'PREDICTED: {model_out_text}')\n\n\n\n#             # After two examples, we break the loop\n\n#             if count == num_examples:\n\n#                 break\n\n\n\n\n\n\n\n\n\n# # We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n\n# def get_model(config, vocab_src_len, vocab_tgt_len):\n\n\n\n#     # Loading model using the 'build_transformer' function.\n\n#     # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n\n#     model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n\n#     return model\n\n\n\n\n\n\n\n\n\n# def get_config():\n\n#     return{\n\n#         'batch_size': 8,\n\n#         'num_epochs': 20,\n\n#         'lr': 10**-4,\n\n#         'seq_len': 350,\n\n#         'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n\n#         'lang_src': 'en',\n\n#         'lang_tgt': 'it',\n\n#         'model_folder': 'weights',\n\n#         'model_basename': 'tmodel_',\n\n#         'preload': None,\n\n#         'tokenizer_file': 'tokenizer_{0}.json',\n\n#         'experiment_name': 'runs/tmodel'\n\n#     }\n\n\n\n\n\n# # Function to construct the path for saving and retrieving model weights\n\n# def get_weights_file_path(config, epoch: str):\n\n#     model_folder = config['model_folder'] # Extracting model folder from the config\n\n#     model_basename = config['model_basename'] # Extracting the base name for model files\n\n#     model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n\n#     return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename\n\n\n\n\n\n\n\n\n\n\n\n\n\n# def train_model(config):\n\n#     # Setting up device to run on GPU to train faster\n\n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n#     print(f\"Using device {device}\")\n\n\n\n#     # Creating model directory to store weights\n\n#     Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n\n\n\n#     # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n\n#     train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n\n\n#     # Initializing model on the GPU using the 'get_model' function\n\n#     model = get_model(config,tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n\n\n\n#     # Tensorboard\n\n#     writer = SummaryWriter(config['experiment_name'])\n\n\n\n#     # Setting up the Adam optimizer with the specified learning rate from the '\n\n#     # config' dictionary plus an epsilon value\n\n#     optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n\n\n\n#     # Initializing epoch and global step variables\n\n#     initial_epoch = 0\n\n#     global_step = 0\n\n\n\n#     # Checking if there is a pre-trained model to load\n\n#     # If true, loads it\n\n#     if config['preload']:\n\n#         model_filename = get_weights_file_path(config, config['preload'])\n\n#         print(f'Preloading model {model_filename}')\n\n#         state = torch.load(model_filename) # Loading model\n\n\n\n#         # Sets epoch to the saved in the state plus one, to resume from where it stopped\n\n#         initial_epoch = state['epoch'] + 1\n\n#         # Loading the optimizer state from the saved model\n\n#         optimizer.load_state_dict(state['optimizer_state_dict'])\n\n#         # Loading the global step state from the saved model\n\n#         global_step = state['global_step']\n\n\n\n#     # Initializing CrossEntropyLoss function for training\n\n#     # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n\n#     # We also apply label_smoothing to prevent overfitting\n\n#     loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.convert_tokens_to_ids('[PAD]'), label_smoothing = 0.1).to(device)\n\n\n\n#     # Initializing training loop\n\n\n\n#     # Iterating over each epoch from the 'initial_epoch' variable up to\n\n#     # the number of epochs informed in the config\n\n#     for epoch in range(initial_epoch, config['num_epochs']):\n\n\n\n#         # Initializing an iterator over the training dataloader\n\n#         # We also use tqdm to display a progress bar\n\n#         batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n\n\n\n#         # For each batch...\n\n#         for batch in batch_iterator:\n\n#             model.train() # Train the model\n\n\n\n#             # Loading input data and masks onto the GPU\n\n#             encoder_input = batch['encoder_input'].to(device)\n\n#             decoder_input = batch['decoder_input'].to(device)\n\n#             encoder_mask = batch['encoder_mask'].to(device)\n\n#             decoder_mask = batch['decoder_mask'].to(device)\n\n\n\n#             # Running tensors through the Transformer\n\n#             encoder_output = model.encode(encoder_input, encoder_mask)\n\n#             decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n\n#             proj_output = model.project(decoder_output)\n\n\n\n#             # Loading the target labels onto the GPU\n\n#             label = batch['label'].to(device)\n\n\n\n#             # Computing loss between model's output and true labels\n\n#             loss = loss_fn(proj_output.view(-1, tokenizer_tgt.vocab_size), label.view(-1))\n\n\n\n#             # Updating progress bar\n\n#             batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n\n\n\n#             writer.add_scalar('train loss', loss.item(), global_step)\n\n#             writer.flush()\n\n\n\n#             # Performing backpropagation\n\n#             loss.backward()\n\n\n\n#             # Updating parameters based on the gradients\n\n#             optimizer.step()\n\n\n\n#             # Clearing the gradients to prepare for the next batch\n\n#             optimizer.zero_grad()\n\n\n\n#             global_step += 1 # Updating global step count\n\n\n\n#         # We run the 'run_validation' function at the end of each epoch\n\n#         # to evaluate model performance\n\n#         run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n\n\n\n#         # Saving model\n\n#         model_filename = get_weights_file_path(config, f'{epoch:02d}')\n\n#         # Writting current model state to the 'model_filename'\n\n#         torch.save({\n\n#             'epoch': epoch, # Current epoch\n\n#             'model_state_dict': model.state_dict(),# Current model state\n\n#             'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n\n#             'global_step': global_step # Current global step\n\n#         }, model_filename)","metadata":{"id":"TLimqv3molVj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# if __name__ == '__main__':\n\n#     warnings.filterwarnings('ignore') # Filtering warnings\n\n#     config = get_config() # Retrieving config settings\n\n#     train_model(config)","metadata":{"id":"-yrLeWgjolYn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"6jUHjIl9olbk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"t2qpgnKOolem","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"64UKw3VRolhn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"YWyzRyU7olk6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"-oMKykC7olnw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"bZjcMFcNolqk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"HaRFmeoiolto","trusted":true},"outputs":[],"execution_count":null}]}